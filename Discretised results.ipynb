{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"regression = 0\ni_m_g = 1\nchannels = 1\ncycle = 0\nfft_ = 1\nimport cv2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom tqdm import tqdm_notebook as tqdm\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torch\n# Neural networks can be constructed using the torch.nn package.\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data.sampler import SequentialSampler, SubsetRandomSampler\nfrom torch.utils.data import Dataset\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.optim as optim\nimport pickle \n\nfor dirname, _, filenames in os.walk('/kaggle/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport gc\n\nimport warnings \nwarnings.filterwarnings('ignore')\nfrom IPython.display import display, clear_output, FileLinks\n","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/lib/kaggle/gcp.py\n/kaggle/input/flicker-dataset/bb_qp38_qp26_5.yuv\n/kaggle/input/flicker-dataset/bb.yuv\n/kaggle/input/flicker-dataset/mr_qp44_qp26_3.yuv\n/kaggle/input/flicker-dataset/bb_qp38_qp26_2.yuv\n/kaggle/input/flicker-dataset/mr_qp38_qp26_5.yuv\n/kaggle/input/flicker-dataset/bb_qp44_qp26_3.yuv\n/kaggle/input/flicker-dataset/mr_qp32_qp26_3.yuv\n/kaggle/input/flicker-dataset/bb_qp26.yuv\n/kaggle/input/flicker-dataset/tr_qp38_qp26_3.yuv\n/kaggle/input/flicker-dataset/tr_qp44_qp26_3.yuv\n/kaggle/input/flicker-dataset/tr.yuv\n/kaggle/input/flicker-dataset/tr_qp38_qp26_5.yuv\n/kaggle/input/flicker-dataset/mr_qp38_qp26_2.yuv\n/kaggle/input/flicker-dataset/tr_qp38_qp26_2.yuv\n/kaggle/input/flicker-dataset/bb_qp32_qp26_3.yuv\n/kaggle/input/flicker-dataset/phase1.mat\n/kaggle/input/flicker-dataset/mr_qp38_qp26_3.yuv\n/kaggle/working/__notebook_source__.ipynb\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with open('/kaggle/input/geekyfile/geekyfile (1)', 'rb') as f:\n#     prediction_dataset = pickle.load(f)\n# prediction_dataset[1]['target'].apply(lambda x : x.data.cpu().numpy())\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a = np.array([1, 2, 3])\n# from tempfile import TemporaryFile\n# outfile = TemporaryFile()\n# np.save(\"outfile\", a)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vid_tr = '/kaggle/input/flicker-dataset/tr.yuv'\nvid = '/kaggle/input/flicker-dataset/tr_qp44_qp26_3.yuv'\n\n%mkdir Y_train\n%mkdir Y_test\n%mkdir Y\n!ls","execution_count":4,"outputs":[{"output_type":"stream","text":"Y  Y_test  Y_train  __notebook_source__.ipynb\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.io\nmat = scipy.io.loadmat('/kaggle/input/flicker-dataset/phase1.mat')\n\nspeed_per_frame = mat['phase1_object_speed'][:, -1] * 10\nflicker_visibility = mat['phase1_task1_flicker_visibility'][:, 34] * 10","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i,x in enumerate(mat['phase1_video_list'][0]):\n#     print(i, x)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(mat['phase1_task1_flicker_visibility'][:, 7])\nplt.plot(mat['phase1_task1_flicker_visibility'][:, 34])","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f5e12babad0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(mat['phase1_object_speed'][:, 1])\nplt.plot(mat['phase1_object_speed'][:, -1])","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f5e12ad0e50>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#os.listdir('/kaggle/input/flicker-dataset')","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lx = []\nlr = 0.001\nr = 20\nf = 1/5\nx = lr/(f+1)\nfor epoch in tqdm_notebook(range(r)):\n    lx.append(lr/(f+1) + lr/(f+1)*f*np.cos(epoch/r*np.pi))\nplt.plot(lx)\n","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"194d99cab55f4a4d92e75eedbf1cdde8"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f5e0adaae90>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAY0AAAD4CAYAAAAQP7oXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXxU9b3/8dcnOwlLAglrgADiElyABAgqYGut0GqD4gIii1UBl9pq21t6b3vb22t/1S5KsciigrhQRL0qdRfqziIJAoJsAVnCGvZ9CXx/f8zBpmOWAyE5k+T9fDzmMWfO+X7P+cxhmHfmrOacQ0RExI+ooAsQEZGaQ6EhIiK+KTRERMQ3hYaIiPim0BAREd9igi6gqqWmprqMjIygyxARqVHy8/N3OOfSwsfX+tDIyMggLy8v6DJERGoUM1tf2nhtnhIREd8UGiIi4ptCQ0REfFNoiIiIbwoNERHxzVdomFlfM1tpZgVmNrqU6WZmY73pS8ysa0V9zexGM1tmZifNLDtsfr/02q80s6tLjM8ysy+8aWPNzM7sbYuIyJmoMDTMLBoYB/QDMoFBZpYZ1qwf0NF7jADG++i7FLge+ChseZnAQKAT0Bd43JsP3nxHlFhW39N4ryIiUkl+ztPoDhQ459YCmNl0IBf4skSbXOAZF7rO+jwzSzazFkBGWX2dc8u9ceHLywWmO+eOAl+ZWQHQ3czWAQ2dc3O9fs8A/YG3Tvtd+/D0p19x6PgJGifGkZwYR+OkOFISY0lJiiO5Xiwx0dqyJyJ1j5/QaAVsLPG6EOjho00rn31LW968UuZ13BsOH/8NZjaC0C8S2rRpU8HiSjftsw2s2nagzOkNE2JonPSvQElOjKVxYhwpSXGkJMbROCmWjNQkzm3agKgobUUTkdrBT2iU9o0Xfuemstr46et3eb7n5ZybBEwCyM7OPqO7TL17fx8OHzvB7kPH2HXwGLsPHWP3oePsPjV80Ht96Bjb9h1h5db97Dp4jMPHT/zbfBokxNClTQpZbVLIaptC5zbJ1I+v9Sfii0gt5efbqxBoXeJ1OrDZZ5s4H339Lq/QGz6deVVKvbho6sXVo2VyPd99jhz/V9Cs3Lqf/PW7yV+/mzGzV+EcRBmc37whWW1Tvn6kp9QrbTOdiEjE8RMaC4COZtYO2ERoJ/UtYW1mAvd6+yx6AHudc1vMrMhH33AzgWlm9gjQktAO78+ccyfMbL+Z5QDzgaHAY77eZTVKiI2mRaN6tGhUj04tG3F911DO7TtynEUb9pC/fjcLN+zmlc838ey80KVd0hrEk+0FSNe2KXRq2ZD4mOjyFiMiEogKQ8M5V2xm9wLvANHAZOfcMjMb5U2fALwJfA8oAA4Bt5XXF8DMriP0pZ8GvGFmi5xzV3vznkFoR3sxcI9z7tQ2n7uAp4F6hHaAV8lO8KrQMCGW3uem0fvc0EUjT5x0oV8iG3azcP1u8tbv4q2lWwGIi4ni4laN6HNuGtd1bUV6SmKQpYuIfM1CBzzVXtnZ2a6mXOV2+74jLNwQ2py1YN1uFm3cA0DP9k0YkJVOvwubk6T9ISJSDcws3zmX/Y3xCo3ItXHXIV75fBMvLyxk/c5DJMZF0+/CFgzIakVOuyY6KktEqoxCowZzzpG/fjcv5RfyxpIt7D9aTKvkegzo2ooBWem0bZIUdIkiUssoNGqJw8dO8O6XW3kpv5BPCnbgHHTLSGFA13S+f3ELGiTEBl2iiNQCCo1aaMvew7zy+SZeyi9kbdFBEmKjuLpTc27ISufSDqlEa/OViJwhhUYt5pxj0cY9vLywkJmLNrPvSDHNGyZwS482DL8sg4b69SEip0mhUUccOX6C2cu3MyNvIx+uKqJRvVhG9G7PsEszdCa6iPim0KiDvijcy5hZq5i9YjuNk+IY2bs9Q3q2JTFO4SEi5VNo1GGfb9jNo7NW89GqIlLrxzGqTwduzWlLQqzOOheR0ik0hLx1u3h01io+LdhJ0wbx3POtcxjYvbUuWSIi36DQkK/NW7uTR95dxWfrdtGiUQL3fvscbsxqTVyM7hEiIiEKDfk3zjnmrNnJX95dycINe2iVXI/7rjyH67umE6sbTInUeQoNKZVzjg9XFfHoe6tYXLiXNo0Tue/KjvTv3FJ3JxSpw8oKDX0r1HFmxhXnNeXVey7jqWHZNEiI4WcvLua7j37Em19sobb/USEip0ehIUAoPK68oBmv/+hyJtyaRWx0FHc/v5A7puaxac/hoMsTkQih0JB/Y2b0vbA5b9x3Ob/6/gXMWbOTqx75kMmffMWJk/rVIVLXKTSkVDHRUdzRqz3v3t+b7u0a87vXv+S6xz9l2ea9QZcmIgFSaEi5WjdOZMrwbjw2qAub9xzmB3/7lD+8tZzDx05U3FlEah2FhlTIzLj2kpbMeqAPN2alM/HDtXx3zId8tKoo6NJEpJopNMS35MQ4HhpwMdNH5BAbFcXQyZ9x/wuL2HngaNCliUg1UWjIactp34Q3f9yL+759Dq8v2cyVj3zIS/mFOjxXpA5QaMgZSYiN5oHvnseb9/XinLT6/OzFxQx+cj7rdhwMujQRqUK+QsPM+prZSjMrMLPRpUw3MxvrTV9iZl0r6mtmjc3sPTNb7T2neOPjzGyKmX1hZovN7IoSfT7w5rXIezSt1LuXSuvYrAEzRvbkwf4X8kXhXq4e8xHj3i/g+ImTQZcmIlWgwtAws2hgHNAPyAQGmVlmWLN+QEfvMQIY76PvaGC2c64jMNt7DXAngHPuIuAq4C9mVrLOwc65zt5j+2m+X6kCUVHGrTltmfXTPnz7/Kb86Z2VXDP2ExZt3BN0aSJylvn5pdEdKHDOrXXOHQOmA7lhbXKBZ1zIPCDZzFpU0DcXmOoNTwX6e8OZhEIELxT2AN+4/olEnmYNExh/axZPDM1m35HjDBg/hwkfruGkTgoUqTX8hEYrYGOJ14XeOD9tyuvbzDm3BcB7PrWpaTGQa2YxZtYOyAJal5jHFG/T1K/NzEor2MxGmFmemeUVFemw0Op2VWYz3v5Jb67u1IyH3lrBbU8v0BFWIrWEn9Ao7Ys5/E/Hstr46RtuMqFwyQPGAHOAYm/aYG+zVS/vMaS0GTjnJjnnsp1z2WlpaRUsTqpCo3qxjLulKw/2v5C5a3fS768fM3fNzqDLEpFK8hMahfz7X/rpwGafbcrru83bhIX3vB3AOVfsnLvf22eRCyQDq71pm7zn/cA0Qpu/JEKZhfZ1vHr3ZdSPj2Hwk/MYM2uVrmElUoP5CY0FQEcza2dmccBAYGZYm5nAUO8oqhxgr7fJqby+M4Fh3vAw4DUAM0s0syRv+Cqg2Dn3pbe5KtUbHwtcAyw9s7ct1SmzZUP+8aPL6d+lFWNmrWbwk/PYtu9I0GWJyBmoMDScc8XAvcA7wHJghnNumZmNMrNRXrM3gbVAAfAEcHd5fb0+DwFXmdlqQkdJPeSNbwosNLPlwC/41yaoeOAdM1sCLAI2ecuSGiApPoZHburMn2+8hMUb99Lvrx/zwUod/CZS0+jOfVLtCrbv595pn7Ni635G9mnPz757nm4xKxJhdOc+iRjnNG3Aq/dcxuAebZj44VpumjiXjbsOBV2WiPig0JBAJMRG8/vrLuJvt3ShYNsBvj/2Y95eujXoskSkAgoNCdQ1F7fkjft6kZGaxKjn8vnNa0s5clz36hCJVAoNCVybJom8NOpSbr+8HVPnrmfA+Dl8pQsfikQkhYZEhLiYKH59TSZPDs1m057DXDP2Y/6xOPx0IBEJmkJDIsp3Mpvx5n29uKBFQ37098/50zsrdO0qkQii0JCI0zK5HtPuzGFgt9aMe38Ndz2fz8GjxRV3FJEqp9CQiBQXE8Ufrr+I31ybyXtfbuOGCXMp3K3DckWCptCQiGVm3HZZO6bc1p3C3YfoP+5T8tfvCroskTpNoSERr8+5abziXfRw0KT5vJxfGHRJInWWQkNqhHOa1ufVey4jOyOFn764mD+8tVxXyxUJgEJDaozkxDim/rA7t+aELj8y8tk8DmgHuUi1UmhIjRIbHcWD/S/if3M78f7KIgY8PkfXrRKpRgoNqZGG9Mxg6m3d2bL3MLnjPuWzr7SDXKQ6KDSkxrq8Yyqv3Xs5yYmxDH5yHjMWbKy4k4hUikJDarR2qUm8cvdl5LRvwn+8vIQHX/9SO8hFqpBCQ2q8RvVimTK8G8MvzeDJT77i9qkL2HfkeNBlidRKCg2pFWKio/jtDzrx/667iE9W7+D6x+ewfqeulCtytik0pFa5pUcbnr29BzsOHOW6x+ewaOOeoEsSqVUUGlLr9OzQhFe/PoN8Hu+v3B50SSK1hkJDaqWM1CReuqsn7dOSuHNqni49InKW+AoNM+trZivNrMDMRpcy3cxsrDd9iZl1raivmTU2s/fMbLX3nOKNjzOzKWb2hZktNrMrSvTJ8sYXeMuzSr17qdWaNkhg+ogcerRvzE9fXMz4D9bgnI6sEqmMCkPDzKKBcUA/IBMYZGaZYc36AR29xwhgvI++o4HZzrmOwGzvNcCdAM65i4CrgL+Y2ak6x3vzP7Wsvqf5fqWOaZAQy5Th3fnBJS15+O0V/O71L3VTJ5FK8PNLoztQ4Jxb65w7BkwHcsPa5ALPuJB5QLKZtaigby4w1RueCvT3hjMJhQjOue3AHiDbm19D59xcF/pz8ZkSfUTKFBcTxZibO/PDy9ox5dN1/Gj65xwtPhF0WSI1kp/QaAWUPNW20Bvnp015fZs557YAeM9NvfGLgVwzizGzdkAW0NrrV1jGvP6NmY0wszwzyysqKvLxFqW2i4oyfn3NBfzn987njSVbuG3KAvbrXA6R0+YnNErbbxD++76sNn76hptMKBDygDHAHKD4dOblnJvknMt2zmWnpaVVsDipK8yMEb078MhNl/DZV7u4eeI8tu87EnRZIjWKn9AoJPSX/inpwGafbcrru83b5IT3vB3AOVfsnLvfOdfZOZcLJAOrvXmlV1CHSIWu75rOU8O7sW7nQa4fP4e1RQeCLkmkxvATGguAjmbWzszigIHAzLA2M4Gh3lFUOcBeb5NTeX1nAsO84WHAawBmlmhmSd7wVUCxc+5Lb377zSzHO2pq6Kk+Iqerz7lp/P3OHA4fO8ENE+bqJEARnyoMDedcMXAv8A6wHJjhnFtmZqPMbJTX7E1gLVAAPAHcXV5fr89DwFVmtprQUVIPeeObAgvNbDnwC2BIiXLuAp70lrMGeOtM3rQIwCWtk3nprktJio/WSYAiPlltP249Ozvb5eXlBV2GRLDt+49w25QFrNy6n4cHXMyArPSKO4nUcmaW75zLDh+vM8KlztNJgCL+KTRE0EmAIn7FBF2ASKQ4dRJgav14Jn/6FUX7j/LITZ2Ji9HfViKnKDRESjh1EmDThvE89NYKDh07weODu5IQGx10aSIRQX9CiYQxM0b16cCD/S/k/ZXbuW3KAg4eLQ66LJGIoNAQKcOtOW1DZ4+v28WtT81n7yFddkREoSFSjuu6pDPulq4s27SPmyfNpWj/0aBLEgmUQkOkAn0vbM6Tw7JZt/MgN0+cy+Y9h4MuSSQwCg0RH3qfm8azt/egaP9Rbpwwl/U7DwZdkkggFBoiPnXLaMy0O3M4dKyYGyfMZdW2/UGXJFLtFBoip+Gi9Ea8MLInADdPnMsXhXsDrkikeik0RE7Tuc0a8OKoniTGxTDoiXl89tWuoEsSqTYKDZEz0LZJEi/d1ZOmDeMZOnk+H63SHSKlblBoiJyhFo3qMWNkT9ql1ueOqXm8vXRr0CWJVDmFhkglpNaPZ/qdOXRq1ZB7pi3klc8LK+4kUoMpNEQqqVFiLM/d3oMe7Rpz/wuLeXbe+qBLEqkyCg2RsyApPobJw7tx5flN+fWrS5n44ZqgSxKpEgoNkbMkITaaCUOyuObiFvzhrRX85d2VupmT1Dq6NLrIWRQbHcVfB3YhKS6Gx/5ZwNHik/yy3/mYWdCliZwVCg2Rsyw6yvjD9RcRHxvFpI/Wcqz4JL+5NlPBIbWCQkOkCkRFGf/zg07ERUfx5CdfcbT4JL/vfyFRUQoOqdl87dMws75mttLMCsxsdCnTzczGetOXmFnXivqaWWMze8/MVnvPKd74WDObamZfmNlyM/tliT4fePNa5D2aVu7ti1QdM+O/vn8Bd1/Rgb9/toGfv7SEE7rvuNRwFYaGmUUD44B+QCYwyMwyw5r1Azp6jxHAeB99RwOznXMdgdnea4AbgXjn3EVAFjDSzDJKLGuwc66z99h+em9XpHqZGT+/+jzu/865vLywkPtfWETxiZNBlyVyxvxsnuoOFDjn1gKY2XQgF/iyRJtc4BkXOlRknpklm1kLIKOcvrnAFV7/qcAHwC8ABySZWQxQDzgG7DvztygSLDPjx9/pSGyM8ce3V3L8xEn+OrALcTE6eFFqHj+f2lbAxhKvC71xftqU17eZc24LgPd8alPTS8BBYAuwAfizc67kFeGmeJumfm1l7Fk0sxFmlmdmeUVFuiaQRIa7rziHX33/At5aupW7n8/naPGJoEsSOW1+QqO0L+bwDbNltfHTN1x34ATQEmgH/NTM2nvTBnubrXp5jyGlzcA5N8k5l+2cy05LS6tgcSLV545e7fnf3E7MWr6dO5/J58hxBYfULH5CoxBoXeJ1OrDZZ5vy+m7zNmHhPZ/aP3EL8LZz7ri3z+JTIBvAObfJe94PTCMUMCI1ypCeGTw84CI+Xl3EbVMWcOhYcdAlifjmJzQWAB3NrJ2ZxQEDgZlhbWYCQ72jqHKAvd4mp/L6zgSGecPDgNe84Q3At715JQE5wAozizGzVAgdYQVcAyw9g/csEribu7XhkZsuYf5XOxk2+TP2HzkedEkivlQYGs65YuBe4B1gOTDDObfMzEaZ2Siv2ZvAWqAAeAK4u7y+Xp+HgKvMbDVwlfcaQkdb1ScUCAuAKc65JUA88I6ZLQEWAZu8ZYnUSNd1SWfsoC4s3LCHIU99xt7DCg6JfFbbr42TnZ3t8vLygi5DpEzvLNvKvdMWcl7zBjz7wx6kJMUFXZIIZpbvnMsOH69j/kQCdnWn5kwcksWqbQcY9MQ8dhw4GnRJImVSaIhEgG+f34ynhmWzbudBBk6ax/Z9R4IuSaRUCg2RCNGrYxpP39adzXsOc/OkeWzZezjokkS+QaEhEkFy2jfh2du7s2P/UW6aOJeNuw4FXZLIv1FoiESYrLaNee6OHuw9dJybJ85l3Y6DQZck8jWFhkgEuqR1MtPuzOHw8RPcNHEuBdsPBF2SCKDQEIlYF7ZqxPQRPTnpHAMnzWXl1v1BlySi0BCJZOc1b8D0ET2JMmPgpLks3bQ36JKkjlNoiES4c5rWZ8bIntSLjeaWJ+axeOOeoEuSOkyhIVIDZKQm8cLInjRKjGXwk/PJX7+r4k4iVUChIVJDtG6cyIyRPUlrEM+Qpz5j3tqdQZckdZBCQ6QGadGoHi+MyKFlcj2GT/mMj1frJmNSvRQaIjVM04YJTB+RQ0aTJG6fmsf7K7ZX3EnkLFFoiNRAqfXj+fudOZzbrD4jns3jnWVbgy5J6giFhkgNlZIUx/N35NCpZSPufn4hry8Jv6GmyNmn0BCpwRrVi+W5O3qQ1SaF+/7+Oa98Xhh0SVLLKTREarj68TE8/cNu5LRvwgMzFvPCgg1BlyS1mEJDpBZIjIth8vBu9O6Yxi9e/oJn564LuiSppRQaIrVEQmw0k4Zm8Z0LmvLr15bx5Mdrgy5JaiGFhkgtEh8TzeODs+h3YXMefGM5Y2evxjkXdFlSi/gKDTPra2YrzazAzEaXMt3MbKw3fYmZda2or5k1NrP3zGy195zijY81s6lm9oWZLTezX5bok+WNL/CWZ5V7+yK1T1xMFI8N6sL1XVvxyHureOitFQoOOWsqDA0ziwbGAf2ATGCQmWWGNesHdPQeI4DxPvqOBmY75zoCs73XADcC8c65i4AsYKSZZXjTxnvzP7Wsvqf3dkXqhpjoKP58wyUMyWnLxI/W8qtXl3LypIJDKs/PL43uQIFzbq1z7hgwHcgNa5MLPONC5gHJZtaigr65wFRveCrQ3xt2QJKZxQD1gGPAPm9+DZ1zc13oz6ZnSvQRkTBRUcbvcjsxqk8Hnp+/gZ++uJjiEyeDLktqOD+h0QrYWOJ1oTfOT5vy+jZzzm0B8J6beuNfAg4CW4ANwJ+dc7u8foVlzEtESmFmjO53Pj+/+jxe+XwTdz+/kKPFJ4IuS2owP6FR2n6D8N+5ZbXx0zdcd+AE0BJoB/zUzNqfzrzMbISZ5ZlZXlGRLugmcs+3zuG312by7pfbuGNqHoeOFQddktRQfkKjEGhd4nU6EH69grLalNd3m7fJCe/51FXXbgHeds4dd85tBz4Fsr15pVdQBwDOuUnOuWznXHZaWpqPtyhS+w2/rB1/vOFiPi3YwbDJn7HvyPGgS5IayE9oLAA6mlk7M4sDBgIzw9rMBIZ6R1HlAHu9TU7l9Z0JDPOGhwGvecMbgG9780oCcoAV3vz2m1mOd9TU0BJ9RMSHm7Jb89igrizauIdbnpjHroPHgi5JapgKQ8M5VwzcC7wDLAdmOOeWmdkoMxvlNXsTWAsUAE8Ad5fX1+vzEHCVma0GrvJeQ+hoq/rAUkKhM8U5t8SbdhfwpLecNcBbZ/i+Reqs71/cgklDslm97QA3T5zLtn1Hgi5JahCr7cdvZ2dnu7y8vKDLEIk4c9fs5I6pC2hSP57n7+hB68aJQZckEcTM8p1z2eHjdUa4SB3Vs0MTnrujB3sPH+fGCXMp2H4g6JKkBlBoiNRhXdqkMH1EDsUnHTdPnMuyzXuDLkkinEJDpI67oEVDZozMIT4mikGT5pG/fnfQJUkEU2iICO3T6jNjVE8aJ8Ux5Kn5zCnYEXRJEqEUGiICQHpKIjNG9aR1SiLDn17ArC+3BV2SRCCFhoh8rWmDBF4YmcMFzRsw8rl8XszbWHEnqVMUGiLyb5IT43j+zhwu7dCEn7+0hHHvF+jS6vI1hYaIfEP9+BieGtaN3M4t+dM7K/mff3ypS6sLADFBFyAikSkuJopHb+pMWv14nvzkK4oOHOWRmy4hPiY66NIkQAoNESlTVJTxq2syadYwgd+/uZydB44yaWg2DRNigy5NAqLNUyJSoTt7t2fMzZ3JW7ebmyfOY7uuV1VnKTRExJf+XVoxeXg31u88yHWPz2FNkS47UhcpNETEt97npjF9RA5Hjp/ghvFz+HyDzh6vaxQaInJaLk5P5uW7LqVBQiy3PDGf91dsr7iT1BoKDRE5bRmpSbx816V0aJrEHc/k6STAOkShISJnJK1BPNNH9KRn+9BJgI9/oJMA6wKFhoicsfrxMUwe3o0fXNKSP76tkwDrAp2nISKVEhcTxZibO5PWIJ6ndBJgrafQEJFKi4oyfn1NJs29kwB3HTjGxKFZOgmwFtLmKRE5a+7s3Z5Hb76EBet2cdOEuWzaczjokuQsU2iIyFl1XZd0Jg/vxqbdh8n92yfkr98VdElyFvkKDTPra2YrzazAzEaXMt3MbKw3fYmZda2or5k1NrP3zGy195zijR9sZotKPE6aWWdv2gfevE5Na1r5VSAiZ1vvc9N45Z5LqR8fw6BJ83k5vzDokuQsqTA0zCwaGAf0AzKBQWaWGdasH9DRe4wAxvvoOxqY7ZzrCMz2XuOce94519k51xkYAqxzzi0qsazBp6Y753RWkUiEOqdpA1695zKyM1L46YuL+cNbyzmhI6tqPD+/NLoDBc65tc65Y8B0IDesTS7wjAuZBySbWYsK+uYCU73hqUD/UpY9CPj7ab0jEYkYyYlxTP1hdwb3aMPED9cy8tk8DhwtDrosqQQ/odEKKHm6Z6E3zk+b8vo2c85tAfCeS9vUdDPfDI0p3qapX5uZlVawmY0wszwzyysqKir7nYlIlYuNjuL3113E73I78f7KIgY8PoeNuw4FXZacIT+hUdoXc/hvzLLa+Olb+kLNegCHnHNLS4we7Jy7COjlPYaU1tc5N8k5l+2cy05LS/OzOBGpYkN7ZvD0bd3YsvcwueM+5bOvtIO8JvITGoVA6xKv04HNPtuU13ebtwkL7zl8/8RAwn5lOOc2ec/7gWmENn+JSA3Rq2Mar95zGcn1Yhn85DxmLNA1q2oaP6GxAOhoZu3MLI7Ql/nMsDYzgaHeUVQ5wF5vk1N5fWcCw7zhYcBrp2ZmZlHAjYT2gZwaF2Nmqd5wLHANUPJXiIjUAO3T6vPK3ZeR074J//HyEh58/UvtIK9BKjwj3DlXbGb3Au8A0cBk59wyMxvlTZ8AvAl8DygADgG3ldfXm/VDwAwzux3YQCgkTukNFDrn1pYYFw+84wVGNDALeOLM3raIBKlRYixThnfjwTeW8+QnX1FQdICxg7roDPIawGr7VSmzs7NdXl5e0GWISBmen7+e37y2jIzUJJ4alk3bJklBlySAmeU757LDx+uMcBEJ1OAebXnm9u7sOHCU3HGfMnfNzqBLknIoNEQkcJd2SOXVuy8jtX48Q56az7T5G4IuScqg0BCRiJCRmsT/3X0pl3dM5T9f+YLfvLaUY8Ungy5Lwig0RCRiNEyI5alh3bjj8nZMnbueGybMYf3Og0GXJSUoNEQkokRHGb+6JpMJt3Zl3Y6DXDP2E15fEn5qmARFoSEiEanvhS14475edGhan3unfc5/vvIFR46fCLqsOk+hISIRq3XjRF4c1ZORfdozbf4G+o/7lILtB4Iuq05TaIhIRIuNjuKX/S5gym3d2L7/KNc+9gkv6f4cgVFoiEiN8K3zmvLmfb24OL0RP3txMQ/MWMRBXWa92ik0RKTGaN4ogWl35vDjKzvyyuebuPZvn/Dl5n1Bl1WnKDREpEaJjjLuv+pcnr+jBweOFNP/8U95bt56avslkSKFQkNEaqRLO6Ty5o97kdO+Cb96dSn3TvucfUeOB11WrafQEJEaK7V+PE8P78bofufz9rKtfH/sxyzeuCfosmo1hYaI1GhRUcaoPh2YMbInJ0/CDRPm8OTHa7W5qoooNESkVshqm8Ib913Ot85ryoNvLOeHTy9gy97DQZdV6yg0RKTWSE6MY+KQLH57bSZz1+7kO5ZOiOMAAAraSURBVH/5kKlz1unOgGeRQkNEahUzY/hl7Xjv/j5kZTTmNzOXMWD8HJZv0aG5Z4NCQ0RqpdaNE5l6Wzf+OrAzG3cd4trHPuHht1fo+lWVpNAQkVrLzMjt3IpZD/Thui6tGP/BGq4e8xGfrN4RdGk1lkJDRGq9lKQ4/nTjJUy7owcG3PrUfB6YsYhdB48FXVqNo9AQkTrj0nNSefsnvbn3W+cwc9FmrvzLB/zfwkIdnnsafIWGmfU1s5VmVmBmo0uZbmY21pu+xMy6VtTXzBqb2Xtmttp7TvHGDzazRSUeJ82sszcty8y+8OY11sys8qtAROqShNhofnb1ebxxXy8yUpN4YMZihjz1me4Q6FOFoWFm0cA4oB+QCQwys8ywZv2Ajt5jBDDeR9/RwGznXEdgtvca59zzzrnOzrnOwBBgnXNukddnvDf/U8vqeyZvWkTkvOYNeHnUpfxvbicWbdzDdx/9iPEfrOH4Cd2XvDx+fml0Bwqcc2udc8eA6UBuWJtc4BkXMg9INrMWFfTNBaZ6w1OB/qUsexDwdwBvfg2dc3Nd6LfkM2X0ERHxJSrKGNIzg1kP9OFb5zXl4bdXcO1jn7BIlyIpk5/QaAVsLPG60Bvnp015fZs557YAeM9NS1n2zXih4fUreeeV0uoAwMxGmFmemeUVFRWV8bZEREKaN0pgwpAsJg7JYs+h41z3+Kf892tLKdp/NOjSIo6f0Chtv0H4XqOy2vjpW/pCzXoAh5xzS0+jjtBI5yY557Kdc9lpaWl+FiciwtWdmvPeA70ZmtOW5+atp9cf/8kf3lyuo6xK8BMahUDrEq/Tgc0+25TXd5u3yenUpqftYfMcyL9+ZZxaRnoFdYiIVEqDhFj+J/dCZj3Qh76dmjPp47X0evif/OmdFew5pPDwExoLgI5m1s7M4gh9mc8MazMTGOodRZUD7PU2OZXXdyYwzBseBrx2amZmFgXcSGgfCPD1Jqz9ZpbjHTU1tGQfEZGzqX1afcYM7MJ79/fmW+c3Zdz7a+j18Ps8+t4q9h6uu/ftMD/HJ5vZ94AxQDQw2Tn3ezMbBeCcm+B9if+N0NFMh4DbnHN5ZfX1xjcBZgBtgA3Ajc65Xd60K4CHnHM5YXVkA08D9YC3gB+5Ct5Adna2y8vLq3hNiIiUY8XWfYx5bzVvL9tKw4QY7uzVnuGXZdAgITbo0qqEmeU757K/Mb62n9Si0BCRs2nppr2MmbWaWcu3kZwYy8jeHRjasy1J8TFBl3ZWKTRERM6ixRv38OisVXywsogmSXGM6tOBW3PaUi8uOujSzgqFhohIFchfv5sxs1bx8eodpDWI5+4rOjCoexsSYmt2eCg0RESq0Py1O3nkvVXM/2oXzRsmMKpPe67rkk6jxJq5z0OhISJSDeas2cEj764ib/1u4mKiuCqzGTd0TadXx1RiomvONWLLCo3atedGRCRgl3ZIpeeoJizbvI+X8gt5bdEm3liyhbQG8VzXpRUDuqZzXvMGQZd5xvRLQ0SkCh0rPsk/V2zn5YWFvL9iO8UnHRe1asQNWen84JKWpCTFBV1iqbR5SkQkYDsOHGXmos28lF/Il1v2ERttXHl+MwZkpXPFeWnERtDmK4WGiEgE+XLzPl5eGNp8tePAMZokxZHbuRU3ZKWT2bJh0OUpNEREItHxEyf5cGURLy8sZPby7Rw7cZILWjQkt3NLerRrTKeWjYiLqf5fINoRLiISgWKjo/hOZjO+k9mM3QeP8Y8lm3k5v5CH3loBQHxMFBenNyKrbWOy2qbQtU0yTerHB1avfmmIiESg7fuOsHDDbvLW7SZ/w26WbtrL8ROh7+t2qUl0bZNCdkYKWW1TOCetPlFRZ/fu19o8JSJSgx05foKlm/aSv343eet3s3D9bnZ69/lokBBD1zahAMlum8IlrZMrfS0shYaISC3inGP9zkPkrw/9Eslft5tV2/fjHEQZXNCiIc/d3uOMD+nVPg0RkVrEzMhITSIjNYkBWaH70+09fJxFG/eQv343K7fuI7kKLmGi0BARqSUa1Yulz7lp9Dm36m5zHTlnkoiISMRTaIiIiG8KDRER8U2hISIivik0RETEN4WGiIj4ptAQERHfFBoiIuJbrb+MiJkVAevPsHsqsOMslnO2qb7KUX2Vo/oqJ9Lra+uc+8ZZgrU+NCrDzPJKu/ZKpFB9laP6Kkf1VU6k11cWbZ4SERHfFBoiIuKbQqN8k4IuoAKqr3JUX+WovsqJ9PpKpX0aIiLim35piIiIbwoNERHxTaEBmFlfM1tpZgVmNrqU6WZmY73pS8ysazXW1trM3jez5Wa2zMx+XEqbK8xsr5kt8h7/XV31ectfZ2ZfeMv+xr11A15/55VYL4vMbJ+Z/SSsTbWuPzObbGbbzWxpiXGNzew9M1vtPaeU0bfcz2oV1vcnM1vh/fu9YmbJZfQt97NQhfX91sw2lfg3/F4ZfYNafy+UqG2dmS0qo2+Vr79Kc87V6QcQDawB2gNxwGIgM6zN94C3AANygPnVWF8LoKs33ABYVUp9VwCvB7gO1wGp5UwPbP2V8m+9ldBJS4GtP6A30BVYWmLcH4HR3vBo4OEy6i/3s1qF9X0XiPGGHy6tPj+fhSqs77fAz3z8+wey/sKm/wX476DWX2Uf+qUB3YEC59xa59wxYDqQG9YmF3jGhcwDks2sRXUU55zb4pxb6A3vB5YDrapj2WdRYOsvzJXAGufcmV4h4Kxwzn0E7AobnQtM9YanAv1L6erns1ol9Tnn3nXOFXsv5wHpZ3u5fpWx/vwIbP2dYmYG3AT8/Wwvt7ooNEJfwBtLvC7km1/KftpUOTPLALoA80uZ3NPMFpvZW2bWqVoLAwe8a2b5ZjailOkRsf6AgZT9nzXI9QfQzDm3BUJ/KABNS2kTKevxh4R+OZamos9CVbrX23w2uYzNe5Gw/noB25xzq8uYHuT680WhEdpkEi78OGQ/baqUmdUHXgZ+4pzbFzZ5IaFNLpcAjwGvVmdtwGXOua5AP+AeM+sdNj0S1l8c8APgxVImB73+/IqE9fhfQDHwfBlNKvosVJXxQAegM7CF0CagcIGvP2AQ5f/KCGr9+abQCP210brE63Rg8xm0qTJmFksoMJ53zv1f+HTn3D7n3AFv+E0g1sxSq6s+59xm73k78AqhzQAlBbr+PP2Ahc65beETgl5/nm2nNtl5z9tLaRP053AYcA0w2Hkb4MP5+CxUCefcNufcCefcSeCJMpYb9PqLAa4HXiirTVDr73QoNGAB0NHM2nl/jQ4EZoa1mQkM9Y4CygH2ntqUUNW8baBPAcudc4+U0aa51w4z607o33VnNdWXZGYNTg0T2mG6NKxZYOuvhDL/wgty/ZUwExjmDQ8DXiuljZ/PapUws77AL4AfOOcOldHGz2ehquoruY/sujKWG9j683wHWOGcKyxtYpDr77QEvSc+Eh6Eju5ZRejIiv/yxo0CRnnDBozzpn8BZFdjbZcT+gm9BFjkPb4XVt+9wDJCR4PMAy6txvrae8td7NUQUevPW34ioRBoVGJcYOuPUHhtAY4T+uv3dqAJMBtY7T039tq2BN4s77NaTfUVENofcOozOCG8vrI+C9VU37PeZ2sJoSBoEUnrzxv/9KnPXIm21b7+KvvQZURERMQ3bZ4SERHfFBoiIuKbQkNERHxTaIiIiG8KDRER8U2hISIivik0RETEt/8PGkU/4Y+vjEEAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# vid_paths = os.listdir('/kaggle/input/flicker-dataset')\n# vid_paths.remove('tr.yuv')\n# vid_paths.remove('phase1.mat')\n# # vid_paths.remove('tr_qp26.yuv')\n# vid_paths.remove('bb_qp26.yuv')\n# vid_paths.remove('bb.yuv')\n# # vid_paths.remove('tr_qp32_qp26_3.yuv')\n# vid_paths","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvid_paths = os.listdir('/kaggle/input/flicker-dataset/')\nvid_paths.remove('tr.yuv')\nvid_paths.remove('phase1.mat')\n# vid_paths.remove('tr_qp26.yuv')\nvid_paths.remove('bb_qp26.yuv')\nvid_paths.remove('bb.yuv')\n# vid_paths.remove('tr_qp32_qp26_3.yuv')\nvid_paths\n","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"['bb_qp38_qp26_5.yuv',\n 'mr_qp44_qp26_3.yuv',\n 'bb_qp38_qp26_2.yuv',\n 'mr_qp38_qp26_5.yuv',\n 'bb_qp44_qp26_3.yuv',\n 'mr_qp32_qp26_3.yuv',\n 'tr_qp38_qp26_3.yuv',\n 'tr_qp44_qp26_3.yuv',\n 'tr_qp38_qp26_5.yuv',\n 'mr_qp38_qp26_2.yuv',\n 'tr_qp38_qp26_2.yuv',\n 'bb_qp32_qp26_3.yuv',\n 'mr_qp38_qp26_3.yuv']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vid_paths = [\n 'tr_qp38_qp26_3.yuv',\n 'tr_qp44_qp26_3.yuv',\n 'tr_qp38_qp26_5.yuv',\n 'tr_qp38_qp26_2.yuv',\n\n 'mr_qp44_qp26_3.yuv',\n 'mr_qp38_qp26_5.yuv',\n 'mr_qp32_qp26_3.yuv',\n 'mr_qp38_qp26_3.yuv',\n             \n            ]\n\nvid_paths","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"['tr_qp38_qp26_3.yuv',\n 'tr_qp44_qp26_3.yuv',\n 'tr_qp38_qp26_5.yuv',\n 'tr_qp38_qp26_2.yuv',\n 'mr_qp44_qp26_3.yuv',\n 'mr_qp38_qp26_5.yuv',\n 'mr_qp32_qp26_3.yuv',\n 'mr_qp38_qp26_3.yuv']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vid_paths = ['tr_qp38_qp26_3.yuv',\n 'tr_qp44_qp26_3.yuv',\n 'tr_qp38_qp26_5.yuv',\n 'tr_qp38_qp26_2.yuv',\n            ]\n\nvid_paths","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"['tr_qp38_qp26_3.yuv',\n 'tr_qp44_qp26_3.yuv',\n 'tr_qp38_qp26_5.yuv',\n 'tr_qp38_qp26_2.yuv']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vid_paths = ['bb_qp38_qp26_5.yuv',\n#  'mr_qp44_qp26_3.yuv',\n#  'tr_qp38_qp26_3.yuv',\n#             ]\n\n# vid_paths","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fft(img):\n    if len(img.shape) == 3:\n        img = img[:, :, 0]\n    f = np.fft.fft2(img)\n    fshift = np.fft.fftshift(f)\n    magnitude_spectrum = 20*np.log(np.abs(fshift))\n\n#     plt.figure(figsize=(5,5))\n#     plt.subplot(121),plt.imshow(img, cmap = 'gray')\n#     plt.title('Input Image'), plt.xticks([]), plt.yticks([])\n#     plt.subplot(122),plt.imshow(magnitude_spectrum, cmap = 'gray')\n#     plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])\n#     plt.show()\n    return magnitude_spectrum\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def enc_(flicker_visibility, flick_flag):\n    if flicker_visibility[flick_flag] /10  < 0.5:\n        enc = 0\n    elif flicker_visibility[flick_flag] /10  < 1:\n        enc = 1\n    elif flicker_visibility[flick_flag] /10  < 1.5:\n        enc = 2\n    elif flicker_visibility[flick_flag] /10  < 2:\n        enc = 3\n    elif flicker_visibility[flick_flag] /10  < 2.5:\n        enc = 4\n    elif flicker_visibility[flick_flag] /10  < 3:\n        enc = 5\n    elif flicker_visibility[flick_flag] /10  < 3.5:\n        enc = 6\n    elif flicker_visibility[flick_flag] /10  < 4:\n        enc = 7\n    elif flicker_visibility[flick_flag] /10  < 4.5:\n        enc = 8\n    elif flicker_visibility[flick_flag] /10  < 5:\n        enc = 9\n    elif flicker_visibility[flick_flag] /10  < 5.5:\n        enc = 10\n    elif flicker_visibility[flick_flag] /10  < 6:\n        enc = 11\n    elif flicker_visibility[flick_flag] /10  < 6.5:\n        enc = 12\n    elif flicker_visibility[flick_flag] /10  < 7:\n        enc = 13\n    elif flicker_visibility[flick_flag] /10  < 7.5:\n        enc = 14\n    elif flicker_visibility[flick_flag] /10  < 8:\n        enc = 15\n    elif flicker_visibility[flick_flag] /10  < 8.5:\n        enc = 16\n    elif flicker_visibility[flick_flag] /10  < 9:\n        enc = 17\n    elif flicker_visibility[flick_flag] /10  < 9.5:\n        enc = 18\n    else :\n        enc = 19\n    return enc","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def enc_(flicker_visibility, flick_flag):\n#     if flicker_visibility[flick_flag] /10  < 1:\n#         enc = 0\n#     elif flicker_visibility[flick_flag] /10  <2:\n#         enc = 1\n#     elif flicker_visibility[flick_flag] /10  < 3:\n#         enc = 2\n#     elif flicker_visibility[flick_flag] /10  < 4:\n#         enc = 3\n#     elif flicker_visibility[flick_flag] /10  < 5:\n#         enc = 4\n#     elif flicker_visibility[flick_flag] /10  < 6:\n#         enc = 5\n#     elif flicker_visibility[flick_flag] /10  < 7:\n#         enc = 6\n#     elif flicker_visibility[flick_flag] /10  < 8:\n#         enc = 7\n#     elif flicker_visibility[flick_flag] /10  < 9:\n#         enc = 8\n#     else :\n#         enc = 9\n#     return enc","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfolder_path = '/kaggle/input/flicker-dataset'\nphase1_video_list = pd.Series(mat['phase1_video_list'][0])\ntrain_names  = []\ntrain_names_fft  = []\ntrain_speeds = []\ntrain_labels = []\ntrain_encoded_labels = []\n\n\ntest_names  = []\ntest_names_fft  = []\ntest_speeds = []\ntest_labels = []\ntest_encoded_labels = []\n\nspeed_dict = {\n    'bb_' : 0,\n    'bmx' : 1,\n    'la_' : 2,\n    'mr_' : 3,\n    'rc_' : 4,\n    'tr_' : 5,\n             } \n\nw, h = 1280, 720\nfor vid in tqdm_notebook(vid_paths):\n    vid_path = os.path.join(folder_path, vid)\n    fid = open(vid_path,'rb');\n    dim = np.fromfile(fid, dtype=np.uint8) # read in entire dataset\n    img_1=np.random.uniform(size=(h, w))*255\n    img_2=np.random.uniform(size=(h, w))*255\n    vid = vid[:-4]\n    \n    idx = phase1_video_list[phase1_video_list == vid].index.values[0]\n    flicker_visibility = mat['phase1_task1_flicker_visibility'][:, idx]\n    speed_per_frame = mat['phase1_object_speed'][:, speed_dict[vid[:3]]]\n    \n    index = 0\n    endindex = w*h\n    nextindex = int(w*h*3/2)\n    train_count=1","execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd6a482f85404facb0aca8d1b9590b96"}},"metadata":{}},{"output_type":"stream","text":"\nCPU times: user 95.5 ms, sys: 907 ms, total: 1 s\nWall time: 4.09 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfolder_path = '/kaggle/input/flicker-dataset'\nphase1_video_list = pd.Series(mat['phase1_video_list'][0])\ntrain_names  = []\ntrain_names_fft  = []\ntrain_speeds = []\ntrain_labels = []\ntrain_encoded_labels = []\n\n\ntest_names  = []\ntest_names_fft  = []\ntest_speeds = []\ntest_labels = []\ntest_encoded_labels = []\n\nspeed_dict = {\n    'bb_' : 0,\n    'bmx' : 1,\n    'la_' : 2,\n    'mr_' : 3,\n    'rc_' : 4,\n    'tr_' : 5,\n             } \n\nw, h = 1280, 720\nfor vid in tqdm_notebook(vid_paths):\n    vid_path = os.path.join(folder_path, vid)\n    fid = open(vid_path,'rb');\n    dim = np.fromfile(fid, dtype=np.uint8) # read in entire dataset\n    img_1=np.random.uniform(size=(h, w))*255\n    img_2=np.random.uniform(size=(h, w))*255\n    vid = vid[:-4]\n    \n    idx = phase1_video_list[phase1_video_list == vid].index.values[0]\n    flicker_visibility = mat['phase1_task1_flicker_visibility'][:, idx]\n    speed_per_frame = mat['phase1_object_speed'][:, speed_dict[vid[:3]]]\n    \n    index = 0\n    endindex = w*h\n    nextindex = int(w*h*3/2)\n    train_count=1\n    for flag in tqdm_notebook(range(int(len(dim) // (h*2*w)))):\n        x=1\n        newdim = dim[index:endindex]\n        newdim = newdim.reshape(x, h, w)    \n\n        index = nextindex\n        endindex = index + w*h\n        nextindex = index + int(w*h*3/2)\n        img = newdim[0]\n\n        \n        \n        fft_img = fft(img)\n        fft_img_1 = fft(img_1)\n        fft_img_2 = fft(img_2)\n        \n        if i_m_g:\n            if channels:\n                new_img_fft = np.array([fft_img, fft_img_1, fft_img_2]).transpose(1,2,0)\n#                 new_img_fft = np.array([fft_img, fft_img, fft_img]).transpose(1,2,0)\n                new_img_fft = Image.fromarray(np.uint8(new_img_fft), 'RGB')\n                new_img = np.array([img, img_1, img_2]).transpose(1,2,0)\n                new_img = Image.fromarray(np.uint8(new_img), 'RGB')\n                \n                \n            else:\n                new_img_fft = np.array(fft_img)#, fft_img_1, fft_img_2])\n                new_img_fft = Image.fromarray(np.uint8(new_img_fft), 'L')\n                \n                new_img = np.array(img)#, fft_img_1, fft_img_2])\n                new_img = Image.fromarray(np.uint8(new_img), 'L')\n        \n        else:\n            new_img_fft = np.array(fft_img)#fft_img_1, fft_img_2])\n            new_img_fft = new_img.transpose(1,2,0)\n            \n            new_img = np.array(img)#fft_img_1, fft_img_2])\n            new_img = new_img.transpose(1,2,0)\n\n        img_2 = img_1.copy()\n        img_1 = img.copy()\n        img = new_img.copy()\n        img_fft = new_img_fft.copy()\n\n        if flag <= 2 :\n            continue\n            \n            \n        flick_flag = flag\n        if train_count <= 4:\n            train_count+=1\n\n            name = \"Y/\" +vid+ \"y\" + str(flag)\n            if i_m_g:\n                img_fft.save(name + 'fft.png')\n                name_fft = name.split('/')[-1] + 'fft.png'\n                \n                img.save(name + '.png')\n                name = name.split('/')[-1] + '.png'\n            else:\n                np.save(name + 'fft', img)\n                name_fft = name.split('/')[-1] + 'fft.npy'\n                \n                np.save(name, img)\n                name = name.split('/')[-1] + '.npy'\n#             plt.imshow(img)\n#             plt.show()\n\n            train_names.append(name)\n            train_names_fft.append(name_fft)\n            train_speeds.append([speed_per_frame[flag-0], speed_per_frame[flag-1], speed_per_frame[flag-2]])\n#             train_speeds.append([speed_per_frame[flag-0], speed_per_frame[flag], speed_per_frame[flag]])\n            train_labels.append(flicker_visibility[flick_flag] )\n            enc = enc_(flicker_visibility, flick_flag)\n            train_encoded_labels.append(enc)\n        else:\n            train_count=1\n\n            name = \"Y/\" +vid+ \"y\" + str(flag)\n            if i_m_g:\n                img_fft.save(name + 'fft.png')\n                name_fft = name.split('/')[-1] + 'fft.png'\n                \n                img.save(name + '.png')\n                name = name.split('/')[-1] + '.png'\n            else:\n                np.save(name + 'fft', img)\n                name_fft = name.split('/')[-1] + 'fft.npy'\n                \n                np.save(name, img)\n                name = name.split('/')[-1] + '.npy'\n#             plt.imshow(img)\n#             plt.show()\n\n            test_names.append(name)\n            test_names_fft.append(name_fft)\n            test_speeds.append([speed_per_frame[flag-0], speed_per_frame[flag-1], speed_per_frame[flag-2]])\n#             test_speeds.append([speed_per_frame[flag-0], speed_per_frame[flag], speed_per_frame[flag]])\n            test_labels.append(flicker_visibility[flick_flag] )\n            enc = enc_(flicker_visibility, flick_flag)\n            test_encoded_labels.append(enc)\n\n#         if flag==4:\n#             break\n","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c89efd1254047588a5e930a46877c43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=225.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06a271a3bc0345cd95c18c7bfde49cb4"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {'Images':train_names, 'Images_fft':train_names_fft, 'labels':train_labels, 'speed' : train_speeds, 'encoded_labels' : train_encoded_labels} \ndata = pd.DataFrame(data)\ndata['labels'] = data['labels'].astype(str)\ndata['nu_labels'] = data['labels'].astype(float)\ndata.head()\n\n\ntrain_data = data.copy()\n\ndata = {'Images':test_names, 'Images_fft':test_names_fft, 'labels':test_labels, 'speed' : test_speeds, 'encoded_labels' : test_encoded_labels} \ndata = pd.DataFrame(data)\ndata['labels'] = data['labels'].astype(str)\ndata['nu_labels'] = data['labels'].astype(float)\ndata.head()\n\n\ntest_data = data.copy()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nvalidation_split = .25\nshuffle_dataset = False\nrandom_seed= 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train_data, test_data], axis=0)\ndata = data.reset_index(drop=True)\ntrain_indices, val_indices = list(data.iloc[:len(train_data), :].index), list(data.iloc[len(train_data):, :].index)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset_size = len(data)\n# indices = list(range(dataset_size))\n# split = int(np.floor(validation_split * dataset_size))\n# if shuffle_dataset :\n#     np.random.seed(random_seed)\n#     np.random.shuffle(indices)\n# train_indices, val_indices = indices[split:], indices[:split]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose(\n    [transforms.ToTensor(),\n#      transforms.Resize(325),\n     transforms.Normalize((0.5), (0.5))])\n\nif i_m_g:\n    transform = transforms.Compose(\n        [transforms.ToTensor(),\n#          transforms.Resize(320),\n         transforms.Normalize((0.5), (0.5))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Arthopod_Dataset(Dataset):\n    def __init__(self, img_data,img_path,transform=None):\n        self.img_path = img_path\n        self.transform = transform\n        self.img_data = img_data\n        \n    def __len__(self):\n        return len(self.img_data)\n    \n    def __getitem__(self, index):\n        img_name = os.path.join(self.img_path,\n                                self.img_data.loc[index, 'Images'])\n        img_name_fft = os.path.join(self.img_path,\n                                self.img_data.loc[index, 'Images_fft'])\n        \n        \n        if i_m_g : \n            image = Image.open(img_name)\n            image = image.resize((224,224))\n            \n            \n            image_fft = Image.open(img_name_fft)\n            image_fft = image_fft.resize((224,224))\n            \n        else:\n            image = np.load(img_name)\n            image = image.reshape( -1, 320, 9)\n            \n            image_fft = np.load(img_name_fft)\n            image_fft = image_fft.reshape( -1, 320, 9)\n            \n            \n            \n            \n            \n#             image =  torch.from_numpy(image)\n\n        if regression:\n            label = torch.tensor(self.img_data.loc[index, 'encoded_labels']) # regression\n        else:\n            label = torch.tensor(self.img_data.loc[index, 'encoded_labels']) # classification\n        speed = torch.tensor(self.img_data.loc[index, 'speed']) / 10\n\n        if self.transform is not None:\n            image = self.transform(image)\n            image_fft = self.transform(image_fft)\n        image.float()\n        image_fft.float()\n        \n        if regression:\n            label.float() # regression\n        if i_m_g:\n            return image, image_fft, label, speed, index\n        else:\n            return image[:6, :, :], image_fft[:6, :, :], label, speed, index\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE_PATH = '/kaggle/working/Y/'\ndataset = Arthopod_Dataset(data,BASE_PATH,transform)\n# dataset. __getitem__(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def npy_loader(path):\n    sample = torch.from_numpy(np.load(path))\n    return sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating PT data samplers and loaders:\n\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalid_sampler = SubsetRandomSampler(val_indices)\n\n\n# train_sampler = SequentialSampler(train_indices)\n# valid_sampler = SequentialSampler(val_indices)\n\n\n\ntrain_sampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n                                           sampler=train_sampler)\nvalidation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                                sampler=valid_sampler)\n\n\n\n# train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n#                                            shuffle=False)\n# validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n#                                                 shuffle=False)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1):\n    for data_t, data_t_fft, target_t, speeds_t, index_t in (validation_loader):\n#         print(target_t.data.cpu().numpy())\n        print(i, len(index_t.data.cpu().numpy() - len(train_indices)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def img_display(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    if i_m_g:\n#         npimg = np.transpose(npimg, (2, 0, 1)) # code 3\n        return npimg\n#     return npimg[0, :, :]\n    return npimg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os.listdir('Y/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataiter = iter(train_loader)\nimages, images_fft, labels, speeds, indexes = dataiter.next()\nnp.transpose(images[0], (2, 0, 1)).shape\nnp.transpose(images[0], (0, 1, 2)).shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(out, labels):\n    _,pred = torch.max(out, dim=1)\n    return torch.sum(pred==labels).item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get some random training images\ndataiter = iter(train_loader)\nimages, images_fft, labels, speeds, indexes = dataiter.next()\narthopod_types = {0: 'Coleoptera', 1: 'Diptera', 2: 'Hymenoptera', 3: 'Lepidoptera'}\n# Viewing data examples used for training\nfig, axis = plt.subplots(2, 2, figsize=(15, 10))\nfor i, ax in enumerate(axis.flat):\n    with torch.no_grad():\n        image, label, speed = images[i], labels[i], speeds[i] \n        image = image[0] # code 1\n        ax.imshow(img_display(image)) # add image\n#         ax.set(title = f\"{[label.item(), speed.item()]}\") # add label\n        ax.set(title = f\"{[label.item()]}\") # add label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get some random training images\ndataiter = iter(train_loader)\nimages, images_fft, labels, speeds, indexes = dataiter.next()\narthopod_types = {0: 'Coleoptera', 1: 'Diptera', 2: 'Hymenoptera', 3: 'Lepidoptera'}\n# Viewing data examples used for training\nfig, axis = plt.subplots(2, 2, figsize=(15, 10))\nfor i, ax in enumerate(axis.flat):\n    with torch.no_grad():\n        image, label, speed = images_fft[i], labels[i], speeds[i] \n        image = image[0] # code 1\n        ax.imshow(img_display(image)) # add image\n#         ax.set(title = f\"{[label.item(), speed.item()]}\") # add label\n        ax.set(title = f\"{[label.item()]}\") # add label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # 3 input image channel, 16 output channels, 3x3 square convolution kernel\n#         self.conv1 = nn.Conv2d(1,16,kernel_size=3,stride=2,padding=1) # code 1\n        self.conv1 = nn.Conv2d(9,16,kernel_size=3,stride=2,padding=1) # code 3\n        self.conv2 = nn.Conv2d(16, 32,kernel_size=3,stride=2, padding=1)\n        self.conv3 = nn.Conv2d(32, 64,kernel_size=3,stride=2, padding=1)\n        self.conv4 = nn.Conv2d(64, 64,kernel_size=3,stride=2, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout2d(0.4)\n        self.batchnorm1 = nn.BatchNorm2d(16)\n        self.batchnorm2 = nn.BatchNorm2d(32)\n        self.batchnorm3 = nn.BatchNorm2d(64)\n        self.fc1 = nn.Linear(64*5*5,64 )\n#         self.fc2 = nn.Linear(512, 256) # regssion\n#         self.fc3 = nn.Linear(256, 4) # regssion\n#         self.fc2 = nn.Linear(512, 64)#classification\n#         self.fc3 = nn.Linear(256, 10)#classification\n        \n        self.linear = nn.Linear(4, 1)\n        self.linear_s = nn.Linear(1, 1)\n        self.linear_s1 = nn.Linear(65, 10)\n        \n        \n        \n        \n        \n    def forward(self, img, speed):\n        img.float()\n        speed.float()\n        img = self.batchnorm1(F.relu(self.conv1(img)))\n        img = self.batchnorm2(F.relu(self.conv2(img)))\n        img = self.dropout(self.batchnorm2(self.pool(img)))\n        img = self.batchnorm3(self.pool(F.relu(self.conv3(img))))\n        img = self.dropout(self.conv4(img))\n        img = img.view(-1, 64*5*5) # Flatten layer\n#         img = self.dropout(self.fc1(img))\n        img = self.fc1(img)\n#         img = self.dropout(self.fc2(img)) # regression \n#         img = self.fc2(img)# classification\n\n#         img = self.linear(self.fc3(img)) # regression \n#         img = self.fc3(img) # classification\n        \n    \n#         img   = self.fc3(img)\n        speed = speed.view(speed.size(0), -1)\n        speed = self.linear_s(speed)\n        speed = speed.view(speed.size(0), -1)\n        img   = img.view(speed.size(0), -1)\n        x = torch.cat([img, speed], dim=1)\n        x = self.linear_s1(x)\n        \n        \n        return x #img\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(out, labels):\n    _,pred = torch.max(out, dim=1)\n    return torch.sum(pred==labels).item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if ~i_m_g:\n    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n\n            vgg16 = models.vgg16(pretrained=True)\n    #         vgg16.load_state_dict(torch.load(\"../input/vgg16bn/vgg16_bn.pth\"))\n            print(vgg16.classifier[6].out_features) # 1000 \n\n\n            # Freeze training for all layers\n            for param in vgg16.features.parameters():\n                param.require_grad = False\n\n            # Newly created modules have require_grad=True by default\n            num_features = vgg16.classifier[6].in_features\n            features = list(vgg16.classifier.children())[:-1] # Remove last layer\n            features.extend([nn.Linear(num_features, 10)]) # Add our layer with 4 outputs\n            vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n            self.model_ft1 = vgg16\n            self.model_ft2 = vgg16\n    #         self.model_ft3 = vgg16\n\n\n\n            self.linear = nn.Linear(4, 1)\n            self.linear_s = nn.Linear(1, 1)\n            self.linear_s1 = nn.Linear(23, 1)\n\n\n\n\n\n        def forward(self, img, speed):\n            img.float()\n            speed.float()\n\n\n            img1 = self.model_ft1(img[ :, :3, :, :])\n            img2 = self.model_ft2(img[ :, 3:6, :, :])\n    #         img3 = self.model_ft3(img[ :, 6:9, :, :])\n\n\n            speed = speed.view(speed.size(0), -1)\n    #         speed = self.linear_s(speed)\n            speed = speed.view(speed.size(0), -1)\n            img1   = img1.view(speed.size(0), -1)\n            img2   = img2.view(speed.size(0), -1)\n    #         img3   = img3.view(speed.size(0), -1)\n    #         x = torch.cat([img1, img2, img3, speed], dim=1)\n            x = torch.cat([img1, img2, speed], dim=1)\n            x = self.linear_s1(x)\n\n\n            return x\n    \nif i_m_g:\n    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n\n            vgg16 = models.vgg16(pretrained=True)\n    #         vgg16.load_state_dict(torch.load(\"../input/vgg16bn/vgg16_bn.pth\"))\n            print(vgg16.classifier[6].out_features) # 1000 \n\n\n            # Freeze training for all layers\n            for param in vgg16.features.parameters():\n                param.require_grad = False\n\n            # Newly created modules have require_grad=True by default\n            num_features = vgg16.classifier[6].in_features\n            features = list(vgg16.classifier.children())[:-1] # Remove last layer\n            features.extend([nn.Linear(num_features, 20)]) # Add our layer with 4 outputs\n            vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n            \n            self.model_ft1 = vgg16\n\n            vgg16_1 = models.vgg16(pretrained=True)\n    #         vgg16.load_state_dict(torch.load(\"../input/vgg16bn/vgg16_bn.pth\"))\n            print(vgg16_1.classifier[6].out_features) # 1000 \n\n            for param in vgg16_1.features.parameters():\n                param.require_grad = False\n\n            # Newly created modules have require_grad=True by default\n            num_features = vgg16_1.classifier[6].in_features\n            features = list(vgg16_1.classifier.children())[:-1] # Remove last layer\n            features.extend([nn.Linear(num_features, 1)]) # Add our layer with 4 outputs\n            vgg16_1.classifier = nn.Sequential(*features) # Replace the model classifier\n            self.model_ft2 = vgg16_1\n            \n            \n            vgg16 = models.vgg16(pretrained=True)\n    #         vgg16.load_state_dict(torch.load(\"../input/vgg16bn/vgg16_bn.pth\"))\n            print(vgg16.classifier[6].out_features) # 1000 \n\n\n            # Freeze training for all layers\n            for param in vgg16.features.parameters():\n                param.require_grad = False\n\n            # Newly created modules have require_grad=True by default\n            num_features = vgg16.classifier[6].in_features\n            features = list(vgg16.classifier.children())[:-1] # Remove last layer\n            features.extend([nn.Linear(num_features, 1)]) # Add our layer with 4 outputs\n            vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n            \n            self.model_ft3 = vgg16\n            \n            \n            \n    #         self.model_ft = models.inception_v3(pretrained=True)\n    #         for param in self.model_ft.parameters():\n    #             param.requires_grad = False\n\n    #         # Parameters of newly constructed modules have requires_grad=True by default\n    #         # Handle the auxilary net\n    #         self.num_ftrs = self.model_ft.AuxLogits.fc.in_features\n    #         self.model_ft.AuxLogits.fc = nn.Linear(self.num_ftrs, 2)\n    #         # Handle the primary net\n    #         num_ftrs = self.model_ft.fc.in_features\n    #         self.model_ft.fc = nn.Linear(self.num_ftrs, 4)\n\n    \n    \n\n            self.linear = nn.Linear(4, 1)\n            self.linear_s = nn.Linear(1, 1)\n            if regression : \n                self.linear_s1 = nn.Linear(25, 1)\n            else:\n                self.linear_s1 = nn.Linear(25, 20)\n\n\n\n\n\n        def forward(self, img_original, img_original_fft, speed):\n            img_original.float()\n            speed.float()\n\n            speed = speed.view(speed.size(0), -1)\n        \n            model1_outs_10 = self.model_ft1(img_original)\n            model1_outs_10   = model1_outs_10.view(speed.size(0), -1)\n            \n            model2_outs_10 = self.model_ft2(img_original)\n            model2_outs_10   = model2_outs_10.view(speed.size(0), -1)\n            \n            model3_outs_1 = self.model_ft3(img_original_fft)\n            model3_outs_1   = model3_outs_1.view(speed.size(0), -1)\n            \n            \n            x = torch.cat([model1_outs_10, speed], dim=1)\n            x = x.view(speed.size(0), -1)\n            x = torch.cat([x, model2_outs_10], dim=1)\n            x = x.view(speed.size(0), -1)\n            x = torch.cat([x, model3_outs_1], dim=1)\n            x = self.linear_s1(x)\n\n\n            return x\n\n\n# print(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for epoch in tqdm_notebook(range(1, 20+1)):\n#     print(0.0001 + 1/((100+epoch**2)*10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"#  + 1/((100+epoch**2)*10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 20\nmodel = Net().to(device)  # On GPU\nif regression:\n    criterion = nn.MSELoss()# regression\nelse :\n    criterion = nn.CrossEntropyLoss()# classification\n\n\nprint_every = 1\nvalid_loss_min = np.Inf\nval_loss = []\nval_acc = []\ntrain_loss = []\ntrain_acc = []\nprediction_dataset = {}\n\nval_target = []\nval_pred = []\n\n\ntotal_step = len(train_loader)\n\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(1, 1, 1) \nfig.suptitle(\"Train - Validation Loss\")\nplt.xlabel('num_epochs', fontsize=12)\nplt.ylabel('loss', fontsize=12)\nplt.legend(loc=2, prop={'size': 60})\nimport matplotlib.patches as mpatches\nred_patch = mpatches.Patch(color='green', label='train')\ngreen_patch = mpatches.Patch(color='red', label='val')\nplt.legend(handles=[red_patch, green_patch])\nax.set_xlim(0, n_epochs+1)\n\nlr = 0.001\nf = 2/5\nr =  n_epochs\n\nloss_reg = [0]*5\ncount = 0\n\nfor i in range(100):\n    if np.std(loss_reg) <= np.mean(loss_reg) / 2:\n        loss_reg.append(i)\n        loss_reg = loss_reg[1:]\n        print(loss_reg)\n\ndata_stack = pd.DataFrame(index = range(len(val_indices) + 1))\ndata_stack['target'] = 0\nfff = 0\n\n\nfor epoch in tqdm_notebook(range(1, n_epochs+1)):\n    if fff:\n        break\n    try:\n        print('lr : ', lr)\n        print(loss_reg)\n        print(prediction_dataset[epoch-1])\n    except:\n        print(\"\")\n    gc.collect()\n    if cycle : \n        LR = lr/(f+1) + lr/(f+1)*f*np.cos(epoch/r*np.pi)\n    else:\n        LR = lr\n#     optimizer = optim.Adam(model.parameters(), lr=LR, )\n    optimizer = optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n#     optimizer = optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08,amsgrad=False)\n    running_loss = 0.0\n    # scheduler.step(epoch)\n    correct = 0\n    total=0\n    print(f'Epoch {epoch}\\n')\n    for batch_idx, (data_,  data_fft_, target_, speeds_, index_) in tqdm_notebook(enumerate(train_loader)):\n        data_ = data_.float()\n        data_fft_ = data_fft_.float()\n        if regression:\n            target_ = target_.float() # regression\n        speeds_ = speeds_.float()\n        data_, data_fft_, target_ , speeds_ = data_.to(device),  data_fft_.to(device), target_.to(device), speeds_.to(device) # on GPU\n        \n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # forward + backward + optimize\n        \n        outputs = model(data_, data_fft_, speeds_)\n#         print(outputs.shape)\n        \n        loss = criterion(outputs, target_)\n        loss.backward()\n        optimizer.step()\n        # print statistics\n        running_loss += loss.item()\n        _,pred = torch.max(outputs, dim=1)\n        correct += torch.sum(pred==target_).item()\n        total += target_.size(0)\n        if (batch_idx) % print_every == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n    train_acc.append(100 * correct / total)\n    train_loss.append(running_loss/total_step)\n    print(f'\\ntrain loss: {np.mean(train_loss):.4f}')\n    batch_loss = 0\n    total_t=0\n    correct_t=0\n    with torch.no_grad():\n        model.eval()\n        for data_t, data_t_fft, target_t, speeds_t, index_t in (validation_loader):\n            data_t.float()\n            data_t_fft.float()\n            if regression:\n                target_t = target_t.float() # regression\n            speeds_t = speeds_t.float()\n            data_t, data_t_fft, target_t, speeds_t = data_t.to(device), data_t_fft.to(device), target_t.to(device), speeds_t.to(device) # on GPU\n            outputs_t = model(data_t, data_t_fft, speeds_t)\n            loss_t = criterion(outputs_t, target_t)\n            batch_loss += loss_t.item()\n            _,pred_t = torch.max(outputs_t, dim=1)\n            correct_t += torch.sum(pred_t==target_t).item()\n            total_t += target_t.size(0)\n            data_stack[epoch] = 0\n            for i,j in zip(np.array(target_t.data.cpu().numpy()), data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), 'target']):\n                if i!=j and epoch >= 2:\n                    print(\"error\")\n                    fff = 1\n            \n            \n            data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), 'target'] = np.array(target_t.data.cpu().numpy())\n            if regression:\n                data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), epoch] = np.array(outputs_t.data.cpu().numpy())\n            else:\n                data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), epoch] = np.array(outputs_t.argmax(axis = 1).data.cpu().numpy())\n            \n            for tar in target_t:\n                val_target.append(tar*10)\n            if regression:\n                predd = outputs_t\n            else:\n                predd = outputs_t.argmax(axis = 1)\n            for pre in predd:\n                val_pred.append(pre*10)\n            \n        prediction_dataset[epoch] = pd.DataFrame({\"target\" : val_target, \"preds\" : val_pred})\n        \n        val_target = []\n        val_pred = []\n        \n        val_acc.append(100 * correct_t / total_t)\n        val_loss.append(batch_loss/len(validation_loader))\n        network_learned = batch_loss < valid_loss_min\n        print(f'validation loss: {np.mean(val_loss):.4f}\\n')\n        # Saving the best weight \n        if network_learned:\n            valid_loss_min = batch_loss\n            torch.save(model.state_dict(), 'model_classification_tutorial.pt')\n            print('Detected network improvement, saving current model')\n    model.train()\n\n\n\n    ax.plot( train_loss, label='train', color='green')\n    ax.plot( val_loss, label='val', color='red')\n    clear_output(wait = True)\n    display(fig)\n    \n    last_reg = np.mean(loss_reg)\n    loss_reg.append(val_loss[-1])\n    loss_reg = loss_reg[1:]\n    count += 1\n    \n    if (np.std(loss_reg) <= np.mean(loss_reg) / 20 and count >= 5) or (np.mean(loss_reg) > last_reg + last_reg/5 and count >= 5):\n        print('reducing lr')\n        if lr > 0.0000001 and epoch > 15:\n            count = 0\n#             lr /= 10\n        \n    \n    \n    plt.pause(0.5)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ep = n_epochs\nn_epochs = 20\n\n# model = Net().to(device)  # On GPU\n# if regression:\n#     criterion = nn.MSELoss()# regression\n# else :\n#     criterion = nn.CrossEntropyLoss()# classification\n\n\nprint_every = 1\nvalid_loss_min = np.Inf\nval_loss = []\nval_acc = []\ntrain_loss = []\ntrain_acc = []\n# prediction_dataset = {}\n\nval_target = []\nval_pred = []\n\n\ntotal_step = len(train_loader)\n\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(1, 1, 1) \nfig.suptitle(\"Train - Validation Loss\")\nplt.xlabel('num_epochs', fontsize=12)\nplt.ylabel('loss', fontsize=12)\nplt.legend(loc=2, prop={'size': 60})\nimport matplotlib.patches as mpatches\nred_patch = mpatches.Patch(color='green', label='train')\ngreen_patch = mpatches.Patch(color='red', label='val')\nplt.legend(handles=[red_patch, green_patch])\nax.set_xlim(0, n_epochs+1)\n\nlr = 0.001/5\nf = 2/5\nr =  n_epochs\n\nloss_reg = [0]*5\ncount = 0\n\nfor i in range(100):\n    if np.std(loss_reg) <= np.mean(loss_reg) / 2:\n        loss_reg.append(i)\n        loss_reg = loss_reg[1:]\n        print(loss_reg)\n\n# data_stack = pd.DataFrame(index = range(len(val_indices) + 1))\n# data_stack['target'] = 0\n# fff = 0\n\n\nfor epoch in tqdm_notebook(range(1, n_epochs+1)):\n    if fff:\n        break\n    try:\n        print('lr : ', lr)\n        print(loss_reg)\n        print(prediction_dataset[epoch-1])\n    except:\n        print(\"\")\n    gc.collect()\n    if cycle : \n        LR = lr/(f+1) + lr/(f+1)*f*np.cos(epoch/r*np.pi)\n    else:\n        LR = lr\n#     optimizer = optim.Adam(model.parameters(), lr=LR, )\n    optimizer = optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n#     optimizer = optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08,amsgrad=False)\n    running_loss = 0.0\n    # scheduler.step(epoch)\n    correct = 0\n    total=0\n    print(f'Epoch {epoch}\\n')\n    for batch_idx, (data_,  data_fft_, target_, speeds_, index_) in tqdm_notebook(enumerate(train_loader)):\n        data_ = data_.float()\n        data_fft_ = data_fft_.float()\n        if regression:\n            target_ = target_.float() # regression\n        speeds_ = speeds_.float()\n        data_, data_fft_, target_ , speeds_ = data_.to(device),  data_fft_.to(device), target_.to(device), speeds_.to(device) # on GPU\n        \n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # forward + backward + optimize\n        \n        outputs = model(data_, data_fft_, speeds_)\n#         print(outputs.shape)\n        \n        loss = criterion(outputs, target_)\n        loss.backward()\n        optimizer.step()\n        # print statistics\n        running_loss += loss.item()\n        _,pred = torch.max(outputs, dim=1)\n        correct += torch.sum(pred==target_).item()\n        total += target_.size(0)\n        if (batch_idx) % print_every == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n    train_acc.append(100 * correct / total)\n    train_loss.append(running_loss/total_step)\n    print(f'\\ntrain loss: {np.mean(train_loss):.4f}')\n    batch_loss = 0\n    total_t=0\n    correct_t=0\n    with torch.no_grad():\n        model.eval()\n        for data_t, data_t_fft, target_t, speeds_t, index_t in (validation_loader):\n            data_t.float()\n            data_t_fft.float()\n            if regression:\n                target_t = target_t.float() # regression\n            speeds_t = speeds_t.float()\n            data_t, data_t_fft, target_t, speeds_t = data_t.to(device), data_t_fft.to(device), target_t.to(device), speeds_t.to(device) # on GPU\n            outputs_t = model(data_t, data_t_fft, speeds_t)\n            loss_t = criterion(outputs_t, target_t)\n            batch_loss += loss_t.item()\n            _,pred_t = torch.max(outputs_t, dim=1)\n            correct_t += torch.sum(pred_t==target_t).item()\n            total_t += target_t.size(0)\n            data_stack[ep+epoch] = 0\n            for i,j in zip(np.array(target_t.data.cpu().numpy()), data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), 'target']):\n                if i!=j and epoch >= 2:\n                    print(\"error\")\n                    fff = 1\n            \n            \n            data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), 'target'] = np.array(target_t.data.cpu().numpy())\n            if regression:\n                data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), ep+epoch] = np.array(outputs_t.data.cpu().numpy())\n            else:\n                data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), ep+epoch] = np.array(outputs_t.argmax(axis = 1).data.cpu().numpy())\n            \n            for tar in target_t:\n                val_target.append(tar*10)\n            if regression:\n                predd = outputs_t\n            else:\n                predd = outputs_t.argmax(axis = 1)\n            for pre in predd:\n                val_pred.append(pre*10)\n            \n        prediction_dataset[ep+epoch] = pd.DataFrame({\"target\" : val_target, \"preds\" : val_pred})\n        \n        val_target = []\n        val_pred = []\n        \n        val_acc.append(100 * correct_t / total_t)\n        val_loss.append(batch_loss/len(validation_loader))\n        network_learned = batch_loss < valid_loss_min\n        print(f'validation loss: {np.mean(val_loss):.4f}\\n')\n        # Saving the best weight \n        if network_learned:\n            valid_loss_min = batch_loss\n            torch.save(model.state_dict(), 'model_classification_tutorial.pt')\n            print('Detected network improvement, saving current model')\n    model.train()\n\n\n\n    ax.plot( train_loss, label='train', color='green')\n    ax.plot( val_loss, label='val', color='red')\n    clear_output(wait = True)\n    display(fig)\n    \n    last_reg = np.mean(loss_reg)\n    loss_reg.append(val_loss[-1])\n    loss_reg = loss_reg[1:]\n    count += 1\n    \n    if (np.std(loss_reg) <= np.mean(loss_reg) / 20 and count >= 5) or (np.mean(loss_reg) > last_reg + last_reg/5 and count >= 5):\n        print('reducing lr')\n        if lr > 0.0000001 and epoch > 15:\n            count = 0\n#             lr /= 10\n        \n    \n    \n    plt.pause(0.5)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ep += n_epochs\nn_epochs = 20\n\n# model = Net().to(device)  # On GPU\n# if regression:\n#     criterion = nn.MSELoss()# regression\n# else :\n#     criterion = nn.CrossEntropyLoss()# classification\n\n\nprint_every = 1\nvalid_loss_min = np.Inf\nval_loss = []\nval_acc = []\ntrain_loss = []\ntrain_acc = []\n# prediction_dataset = {}\n\nval_target = []\nval_pred = []\n\n\ntotal_step = len(train_loader)\n\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(1, 1, 1) \nfig.suptitle(\"Train - Validation Loss\")\nplt.xlabel('num_epochs', fontsize=12)\nplt.ylabel('loss', fontsize=12)\nplt.legend(loc=2, prop={'size': 60})\nimport matplotlib.patches as mpatches\nred_patch = mpatches.Patch(color='green', label='train')\ngreen_patch = mpatches.Patch(color='red', label='val')\nplt.legend(handles=[red_patch, green_patch])\nax.set_xlim(0, n_epochs+1)\n\nlr = 0.001/10\nf = 2/5\nr =  n_epochs\n\nloss_reg = [0]*5\ncount = 0\n\nfor i in range(100):\n    if np.std(loss_reg) <= np.mean(loss_reg) / 2:\n        loss_reg.append(i)\n        loss_reg = loss_reg[1:]\n        print(loss_reg)\n\n# data_stack = pd.DataFrame(index = range(len(val_indices) + 1))\n# data_stack['target'] = 0\n# fff = 0\n\n\nfor epoch in tqdm_notebook(range(1, n_epochs+1)):\n    if fff:\n        break\n    try:\n        print('lr : ', lr)\n        print(loss_reg)\n        print(prediction_dataset[epoch-1])\n    except:\n        print(\"\")\n    gc.collect()\n    if cycle : \n        LR = lr/(f+1) + lr/(f+1)*f*np.cos(epoch/r*np.pi)\n    else:\n        LR = lr\n#     optimizer = optim.Adam(model.parameters(), lr=LR, )\n    optimizer = optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n#     optimizer = optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08,amsgrad=False)\n    running_loss = 0.0\n    # scheduler.step(epoch)\n    correct = 0\n    total=0\n    print(f'Epoch {epoch}\\n')\n    for batch_idx, (data_,  data_fft_, target_, speeds_, index_) in tqdm_notebook(enumerate(train_loader)):\n        data_ = data_.float()\n        data_fft_ = data_fft_.float()\n        if regression:\n            target_ = target_.float() # regression\n        speeds_ = speeds_.float()\n        data_, data_fft_, target_ , speeds_ = data_.to(device),  data_fft_.to(device), target_.to(device), speeds_.to(device) # on GPU\n        \n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # forward + backward + optimize\n        \n        outputs = model(data_, data_fft_, speeds_)\n#         print(outputs.shape)\n        \n        loss = criterion(outputs, target_)\n        loss.backward()\n        optimizer.step()\n        # print statistics\n        running_loss += loss.item()\n        _,pred = torch.max(outputs, dim=1)\n        correct += torch.sum(pred==target_).item()\n        total += target_.size(0)\n        if (batch_idx) % print_every == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n    train_acc.append(100 * correct / total)\n    train_loss.append(running_loss/total_step)\n    print(f'\\ntrain loss: {np.mean(train_loss):.4f}')\n    batch_loss = 0\n    total_t=0\n    correct_t=0\n    with torch.no_grad():\n        model.eval()\n        for data_t, data_t_fft, target_t, speeds_t, index_t in (validation_loader):\n            data_t.float()\n            data_t_fft.float()\n            if regression:\n                target_t = target_t.float() # regression\n            speeds_t = speeds_t.float()\n            data_t, data_t_fft, target_t, speeds_t = data_t.to(device), data_t_fft.to(device), target_t.to(device), speeds_t.to(device) # on GPU\n            outputs_t = model(data_t, data_t_fft, speeds_t)\n            loss_t = criterion(outputs_t, target_t)\n            batch_loss += loss_t.item()\n            _,pred_t = torch.max(outputs_t, dim=1)\n            correct_t += torch.sum(pred_t==target_t).item()\n            total_t += target_t.size(0)\n            data_stack[ep+epoch] = 0\n            for i,j in zip(np.array(target_t.data.cpu().numpy()), data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), 'target']):\n                if i!=j and epoch >= 2:\n                    print(\"error\")\n                    fff = 1\n            \n            \n            data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), 'target'] = np.array(target_t.data.cpu().numpy())\n            if regression:\n                data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), ep+epoch] = np.array(outputs_t.data.cpu().numpy())\n            else:\n                data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), ep+epoch] = np.array(outputs_t.argmax(axis = 1).data.cpu().numpy())\n            \n            for tar in target_t:\n                val_target.append(tar*10)\n            if regression:\n                predd = outputs_t\n            else:\n                predd = outputs_t.argmax(axis = 1)\n            for pre in predd:\n                val_pred.append(pre*10)\n            \n        prediction_dataset[ep+epoch] = pd.DataFrame({\"target\" : val_target, \"preds\" : val_pred})\n        \n        val_target = []\n        val_pred = []\n        \n        val_acc.append(100 * correct_t / total_t)\n        val_loss.append(batch_loss/len(validation_loader))\n        network_learned = batch_loss < valid_loss_min\n        print(f'validation loss: {np.mean(val_loss):.4f}\\n')\n        # Saving the best weight \n        if network_learned:\n            valid_loss_min = batch_loss\n            torch.save(model.state_dict(), 'model_classification_tutorial.pt')\n            print('Detected network improvement, saving current model')\n    model.train()\n\n\n\n    ax.plot( train_loss, label='train', color='green')\n    ax.plot( val_loss, label='val', color='red')\n    clear_output(wait = True)\n    display(fig)\n    \n    last_reg = np.mean(loss_reg)\n    loss_reg.append(val_loss[-1])\n    loss_reg = loss_reg[1:]\n    count += 1\n    \n    if (np.std(loss_reg) <= np.mean(loss_reg) / 20 and count >= 5) or (np.mean(loss_reg) > last_reg + last_reg/5 and count >= 5):\n        print('reducing lr')\n        if lr > 0.0000001 and epoch > 15:\n            count = 0\n#             lr /= 10\n        \n    \n    \n    plt.pause(0.5)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ep += n_epochs\nn_epochs = 20\n\n# model = Net().to(device)  # On GPU\n# if regression:\n#     criterion = nn.MSELoss()# regression\n# else :\n#     criterion = nn.CrossEntropyLoss()# classification\n\n\nprint_every = 1\nvalid_loss_min = np.Inf\nval_loss = []\nval_acc = []\ntrain_loss = []\ntrain_acc = []\n# prediction_dataset = {}\n\nval_target = []\nval_pred = []\n\n\ntotal_step = len(train_loader)\n\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(1, 1, 1) \nfig.suptitle(\"Train - Validation Loss\")\nplt.xlabel('num_epochs', fontsize=12)\nplt.ylabel('loss', fontsize=12)\nplt.legend(loc=2, prop={'size': 60})\nimport matplotlib.patches as mpatches\nred_patch = mpatches.Patch(color='green', label='train')\ngreen_patch = mpatches.Patch(color='red', label='val')\nplt.legend(handles=[red_patch, green_patch])\nax.set_xlim(0, n_epochs+1)\n\nlr = 0.001/50\nf = 2/5\nr =  n_epochs\n\nloss_reg = [0]*5\ncount = 0\n\nfor i in range(100):\n    if np.std(loss_reg) <= np.mean(loss_reg) / 2:\n        loss_reg.append(i)\n        loss_reg = loss_reg[1:]\n        print(loss_reg)\n\n# data_stack = pd.DataFrame(index = range(len(val_indices) + 1))\n# data_stack['target'] = 0\n# fff = 0\n\n\nfor epoch in tqdm_notebook(range(1, n_epochs+1)):\n    if fff:\n        break\n    try:\n        print('lr : ', lr)\n        print(loss_reg)\n        print(prediction_dataset[epoch-1])\n    except:\n        print(\"\")\n    gc.collect()\n    if cycle : \n        LR = lr/(f+1) + lr/(f+1)*f*np.cos(epoch/r*np.pi)\n    else:\n        LR = lr\n#     optimizer = optim.Adam(model.parameters(), lr=LR, )\n    optimizer = optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n#     optimizer = optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08,amsgrad=False)\n    running_loss = 0.0\n    # scheduler.step(epoch)\n    correct = 0\n    total=0\n    print(f'Epoch {epoch}\\n')\n    for batch_idx, (data_,  data_fft_, target_, speeds_, index_) in tqdm_notebook(enumerate(train_loader)):\n        data_ = data_.float()\n        data_fft_ = data_fft_.float()\n        if regression:\n            target_ = target_.float() # regression\n        speeds_ = speeds_.float()\n        data_, data_fft_, target_ , speeds_ = data_.to(device),  data_fft_.to(device), target_.to(device), speeds_.to(device) # on GPU\n        \n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # forward + backward + optimize\n        \n        outputs = model(data_, data_fft_, speeds_)\n#         print(outputs.shape)\n        \n        loss = criterion(outputs, target_)\n        loss.backward()\n        optimizer.step()\n        # print statistics\n        running_loss += loss.item()\n        _,pred = torch.max(outputs, dim=1)\n        correct += torch.sum(pred==target_).item()\n        total += target_.size(0)\n        if (batch_idx) % print_every == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n    train_acc.append(100 * correct / total)\n    train_loss.append(running_loss/total_step)\n    print(f'\\ntrain loss: {np.mean(train_loss):.4f}')\n    batch_loss = 0\n    total_t=0\n    correct_t=0\n    with torch.no_grad():\n        model.eval()\n        for data_t, data_t_fft, target_t, speeds_t, index_t in (validation_loader):\n            data_t.float()\n            data_t_fft.float()\n            if regression:\n                target_t = target_t.float() # regression\n            speeds_t = speeds_t.float()\n            data_t, data_t_fft, target_t, speeds_t = data_t.to(device), data_t_fft.to(device), target_t.to(device), speeds_t.to(device) # on GPU\n            outputs_t = model(data_t, data_t_fft, speeds_t)\n            loss_t = criterion(outputs_t, target_t)\n            batch_loss += loss_t.item()\n            _,pred_t = torch.max(outputs_t, dim=1)\n            correct_t += torch.sum(pred_t==target_t).item()\n            total_t += target_t.size(0)\n            data_stack[ep+epoch] = 0\n            for i,j in zip(np.array(target_t.data.cpu().numpy()), data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), 'target']):\n                if i!=j and epoch >= 2:\n                    print(\"error\")\n                    fff = 1\n            \n            \n            data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), 'target'] = np.array(target_t.data.cpu().numpy())\n            if regression:\n                data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), ep+epoch] = np.array(outputs_t.data.cpu().numpy())\n            else:\n                data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), ep+epoch] = np.array(outputs_t.argmax(axis = 1).data.cpu().numpy())\n            \n            for tar in target_t:\n                val_target.append(tar*10)\n            if regression:\n                predd = outputs_t\n            else:\n                predd = outputs_t.argmax(axis = 1)\n            for pre in predd:\n                val_pred.append(pre*10)\n            \n        prediction_dataset[ep+epoch] = pd.DataFrame({\"target\" : val_target, \"preds\" : val_pred})\n        \n        val_target = []\n        val_pred = []\n        \n        val_acc.append(100 * correct_t / total_t)\n        val_loss.append(batch_loss/len(validation_loader))\n        network_learned = batch_loss < valid_loss_min\n        print(f'validation loss: {np.mean(val_loss):.4f}\\n')\n        # Saving the best weight \n        if network_learned:\n            valid_loss_min = batch_loss\n            torch.save(model.state_dict(), 'model_classification_tutorial.pt')\n            print('Detected network improvement, saving current model')\n    model.train()\n\n\n\n    ax.plot( train_loss, label='train', color='green')\n    ax.plot( val_loss, label='val', color='red')\n    clear_output(wait = True)\n    display(fig)\n    \n    last_reg = np.mean(loss_reg)\n    loss_reg.append(val_loss[-1])\n    loss_reg = loss_reg[1:]\n    count += 1\n    \n    if (np.std(loss_reg) <= np.mean(loss_reg) / 20 and count >= 5) or (np.mean(loss_reg) > last_reg + last_reg/5 and count >= 5):\n        print('reducing lr')\n        if lr > 0.0000001 and epoch > 15:\n            count = 0\n#             lr /= 10\n        \n    \n    \n    plt.pause(0.5)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ep += n_epochs\nn_epochs = 20\n\n# model = Net().to(device)  # On GPU\n# if regression:\n#     criterion = nn.MSELoss()# regression\n# else :\n#     criterion = nn.CrossEntropyLoss()# classification\n\n\nprint_every = 1\nvalid_loss_min = np.Inf\nval_loss = []\nval_acc = []\ntrain_loss = []\ntrain_acc = []\n# prediction_dataset = {}\n\nval_target = []\nval_pred = []\n\n\ntotal_step = len(train_loader)\n\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(1, 1, 1) \nfig.suptitle(\"Train - Validation Loss\")\nplt.xlabel('num_epochs', fontsize=12)\nplt.ylabel('loss', fontsize=12)\nplt.legend(loc=2, prop={'size': 60})\nimport matplotlib.patches as mpatches\nred_patch = mpatches.Patch(color='green', label='train')\ngreen_patch = mpatches.Patch(color='red', label='val')\nplt.legend(handles=[red_patch, green_patch])\nax.set_xlim(0, n_epochs+1)\n\nlr = 0.001/100\nf = 2/5\nr =  n_epochs\n\nloss_reg = [0]*5\ncount = 0\n\nfor i in range(100):\n    if np.std(loss_reg) <= np.mean(loss_reg) / 2:\n        loss_reg.append(i)\n        loss_reg = loss_reg[1:]\n        print(loss_reg)\n\n# data_stack = pd.DataFrame(index = range(len(val_indices) + 1))\n# data_stack['target'] = 0\n# fff = 0\n\n\nfor epoch in tqdm_notebook(range(1, n_epochs+1)):\n    if fff:\n        break\n    try:\n        print('lr : ', lr)\n        print(loss_reg)\n        print(prediction_dataset[epoch-1])\n    except:\n        print(\"\")\n    gc.collect()\n    if cycle : \n        LR = lr/(f+1) + lr/(f+1)*f*np.cos(epoch/r*np.pi)\n    else:\n        LR = lr\n#     optimizer = optim.Adam(model.parameters(), lr=LR, )\n    optimizer = optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n#     optimizer = optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08,amsgrad=False)\n    running_loss = 0.0\n    # scheduler.step(epoch)\n    correct = 0\n    total=0\n    print(f'Epoch {epoch}\\n')\n    for batch_idx, (data_,  data_fft_, target_, speeds_, index_) in tqdm_notebook(enumerate(train_loader)):\n        data_ = data_.float()\n        data_fft_ = data_fft_.float()\n        if regression:\n            target_ = target_.float() # regression\n        speeds_ = speeds_.float()\n        data_, data_fft_, target_ , speeds_ = data_.to(device),  data_fft_.to(device), target_.to(device), speeds_.to(device) # on GPU\n        \n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # forward + backward + optimize\n        \n        outputs = model(data_, data_fft_, speeds_)\n#         print(outputs.shape)\n        \n        loss = criterion(outputs, target_)\n        loss.backward()\n        optimizer.step()\n        # print statistics\n        running_loss += loss.item()\n        _,pred = torch.max(outputs, dim=1)\n        correct += torch.sum(pred==target_).item()\n        total += target_.size(0)\n        if (batch_idx) % print_every == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n    train_acc.append(100 * correct / total)\n    train_loss.append(running_loss/total_step)\n    print(f'\\ntrain loss: {np.mean(train_loss):.4f}')\n    batch_loss = 0\n    total_t=0\n    correct_t=0\n    with torch.no_grad():\n        model.eval()\n        for data_t, data_t_fft, target_t, speeds_t, index_t in (validation_loader):\n            data_t.float()\n            data_t_fft.float()\n            if regression:\n                target_t = target_t.float() # regression\n            speeds_t = speeds_t.float()\n            data_t, data_t_fft, target_t, speeds_t = data_t.to(device), data_t_fft.to(device), target_t.to(device), speeds_t.to(device) # on GPU\n            outputs_t = model(data_t, data_t_fft, speeds_t)\n            loss_t = criterion(outputs_t, target_t)\n            batch_loss += loss_t.item()\n            _,pred_t = torch.max(outputs_t, dim=1)\n            correct_t += torch.sum(pred_t==target_t).item()\n            total_t += target_t.size(0)\n            data_stack[ep+epoch] = 0\n            for i,j in zip(np.array(target_t.data.cpu().numpy()), data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), 'target']):\n                if i!=j and epoch >= 2:\n                    print(\"error\")\n                    fff = 1\n            \n            \n            data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), 'target'] = np.array(target_t.data.cpu().numpy())\n            if regression:\n                data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), ep+epoch] = np.array(outputs_t.data.cpu().numpy())\n            else:\n                data_stack.loc[index_t.data.cpu().numpy() - len(train_indices), ep+epoch] = np.array(outputs_t.argmax(axis = 1).data.cpu().numpy())\n            \n            for tar in target_t:\n                val_target.append(tar*10)\n            if regression:\n                predd = outputs_t\n            else:\n                predd = outputs_t.argmax(axis = 1)\n            for pre in predd:\n                val_pred.append(pre*10)\n            \n        prediction_dataset[ep+epoch] = pd.DataFrame({\"target\" : val_target, \"preds\" : val_pred})\n        \n        val_target = []\n        val_pred = []\n        \n        val_acc.append(100 * correct_t / total_t)\n        val_loss.append(batch_loss/len(validation_loader))\n        network_learned = batch_loss < valid_loss_min\n        print(f'validation loss: {np.mean(val_loss):.4f}\\n')\n        # Saving the best weight \n        if network_learned:\n            valid_loss_min = batch_loss\n            torch.save(model.state_dict(), 'model_classification_tutorial.pt')\n            print('Detected network improvement, saving current model')\n    model.train()\n\n\n\n    ax.plot( train_loss, label='train', color='green')\n    ax.plot( val_loss, label='val', color='red')\n    clear_output(wait = True)\n    display(fig)\n    \n    last_reg = np.mean(loss_reg)\n    loss_reg.append(val_loss[-1])\n    loss_reg = loss_reg[1:]\n    count += 1\n    \n    if (np.std(loss_reg) <= np.mean(loss_reg) / 20 and count >= 5) or (np.mean(loss_reg) > last_reg + last_reg/5 and count >= 5):\n        print('reducing lr')\n        if lr > 0.0000001 and epoch > 15:\n            count = 0\n#             lr /= 10\n        \n    \n    \n    plt.pause(0.5)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_reg = [344.7271270751953,\n 341.93824513753253,\n 342.8001022338867,\n 342.79945373535156,\n 354.19374593098956]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.std(loss_reg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(loss_reg) /50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred = pd.DataFrame(prediction_dataset)\n# pred.to_csv(\"pred.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, len(prediction_dataset)+1):\n    tar = prediction_dataset[i]['target'].apply(lambda x : x.data.cpu().numpy())\n    pre = prediction_dataset[i]['preds'].apply(lambda x : x.data.cpu().numpy())\n    fig = plt.figure(figsize=(5,5))\n    plt.plot( tar, label='target')\n    plt.plot( pre, label='preds')\n    plt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndictionary = {'geek': 1, 'supergeek': True, 4: 'geeky'} \n  \ntry: \n    geeky_file = open('geekyfile', 'wb') \n    pickle.dump(prediction_dataset, geeky_file) \n    geeky_file.close() \n\nexcept: \n    print(\"Something went wrong\")\n    \nwith open('geekyfile', 'rb') as f:\n    x = pickle.load(f)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FileLinks('.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_dataset[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,10))\nplt.title(\"Train - Validation Loss\")\nplt.plot( train_loss, label='train')\nplt.plot( val_loss, label='validation')\nplt.xlabel('num_epochs', fontsize=12)\nplt.ylabel('loss', fontsize=12)\nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"speeds_t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if regression:\n    print(outputs_t)\nelse:\n    print(outputs_t.argmax(axis = 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy(outputs_t, target_t) #/len(target_t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre = np.array(prediction_dataset[i]['preds'].apply(lambda x : x.data.cpu().numpy()))\ntar = np.array(prediction_dataset[i]['target'].apply(lambda x : x.data.cpu().numpy()))\npre","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy(pre, tar) /len(tar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_stack.to_csv('data_stack_20_1_f20.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), \"model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data_stack\n\nfrom sklearn.model_selection  import train_test_split\nfrom sklearn.metrics  import accuracy_score, mean_squared_error\nprint(\"regression :\")\n\n    \ndef xgg_1(data, i):\n    score=0\n    count=0\n#     print(len(data))\n    for j in range(1, len(data)):\n        count += 1\n        train_1 = data.iloc[:j, :]\n        train_2 = data.iloc[j+1:, :]\n        test = data.iloc[j:j+1, :]\n\n        train = pd.concat([train_1, train_2], axis=0)\n\n\n\n        train_x = train.iloc[:, i:]\n        test_x = test.iloc[:, i:]\n\n        y_train = train.iloc[:, 0]*5\n        y_test = test.iloc[:, 0]*5\n        from xgboost import XGBClassifier, XGBRegressor\n        from catboost import CatBoostClassifier, CatBoostRegressor\n        xg = XGBRegressor()\n#         xg = CatBoostRegressor(verbose=False)\n        xg.fit(train_x, y_train)\n        y_pred = xg.predict(test_x)\n#         accuracy_score(y_test, y_pred)\n        score_ = np.sqrt(mean_squared_error(y_test, y_pred))\n    \n        score_ = (mean_absolute_error(y_test, y_pred))\n    \n#         print(\"iter\",j, score_)\n    #     plt.plot(range(len(y_test)), y_test, label='target')\n    #     plt.plot(range(len(y_pred)), y_pred, label='pred')\n    #     plt.legend(loc='best')\n    #     plt.title(str(i)+\" \"+str(np.sqrt(mean_squared_error(y_test, y_pred))))\n    #     plt.show()\n\n        score += score_\n\n        if len(y_pred.shape) == 2:\n            y_pred = y_pred.reshape(1, -1)[0]\n    #     return pd.DataFrame({\"pred\" : y_pred, \"test\" : y_test}), np.sqrt(mean_squared_error(y_test, y_pred))\n    print(score / count)\n    return score / count\n    \nfor i in range(50, 100, 5):\n    try:\n        xgg_1(data, i)\n    except:\n        continue\n        \n        \nfrom sklearn.model_selection  import train_test_split\nfrom sklearn.metrics  import accuracy_score, mean_squared_error\n\nprint(\"classification :\")\n    \ndef xgg_1(data, i):\n    score=0\n    count=0\n#     print(len(data))\n    for j in range(1, len(data)):\n        count += 1\n        train_1 = data.iloc[:j, :]\n        train_2 = data.iloc[j+1:, :]\n        test = data.iloc[j:j+1, :]\n\n        train = pd.concat([train_1, train_2], axis=0)\n\n\n\n        train_x = train.iloc[:, i:]\n        test_x = test.iloc[:, i:]\n\n        y_train = train.iloc[:, 0]*5\n        y_test = test.iloc[:, 0]*5\n        from xgboost import XGBClassifier, XGBRegressor\n        from catboost import CatBoostClassifier, CatBoostRegressor\n        xg = XGBClassifier()\n#         xg = CatBoostRegressor(verbose=False)\n        xg.fit(train_x, y_train)\n        y_pred = xg.predict(test_x)\n#         accuracy_score(y_test, y_pred)\n        score_ = np.sqrt(mean_squared_error(y_test, y_pred))\n    \n        score_ = (mean_absolute_error(y_test, y_pred))\n    \n#         print(\"iter\",j, score_)\n    #     plt.plot(range(len(y_test)), y_test, label='target')\n    #     plt.plot(range(len(y_pred)), y_pred, label='pred')\n    #     plt.legend(loc='best')\n    #     plt.title(str(i)+\" \"+str(np.sqrt(mean_squared_error(y_test, y_pred))))\n    #     plt.show()\n\n        score += score_\n\n        if len(y_pred.shape) == 2:\n            y_pred = y_pred.reshape(1, -1)[0]\n    #     return pd.DataFrame({\"pred\" : y_pred, \"test\" : y_test}), np.sqrt(mean_squared_error(y_test, y_pred))\n    print(score / count)\n    return score / count\n    \nfor i in range(50, 100, 5):\n    try:\n        xgg_1(data, i)\n    except:\n        continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FileLinks('.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**what is cross entropy loss**\n**what is YUV**\n**what is FFT for an image**\n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}